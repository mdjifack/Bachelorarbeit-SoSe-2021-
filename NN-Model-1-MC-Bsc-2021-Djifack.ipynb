{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bachelorarbait-2021\n",
    "# Author: Michel Bosris Djifack\n",
    "# Matrikelnummer:7103963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentiment analysis program will be designed to make predictions about the english written expressions to rank them and \n",
    "# determine which ones are in favor of the coronavirus vaccine and which are against.\n",
    "# Method: NN (Neural Network)\n",
    "\n",
    "# ===> three classes (Multiclass) with NN\n",
    "\n",
    "\n",
    "# NN-Prototype-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m------------------- All libraries have been successfully imported.------------------- \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import all libraries (Math-function, diagram-visualisation, regex, document and NLP functions)\n",
    "\n",
    "\"\"\"\n",
    "# NLP Libraries\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Activation, BatchNormalization,Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "# Math, documents and visualisation Libraries\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "print(f\"{Fore.MAGENTA}------------------- All libraries have been successfully imported.------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m------------------- The document has been successfully uploaded. ------------------- \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "upload The dataset, open it and check it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# upload the DataSet\n",
    "file = open('covidVaccineAdvice_mldata_d1.csv',encoding=\"utf-8\")\n",
    "data = pd.read_csv(file,delimiter=\";\")\n",
    "print(f\"{Fore.MAGENTA}------------------- The document has been successfully uploaded. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m -------------------  overview of the dataset ------------------- \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>We need to be vaccinated to protect all person...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>it is a pleasure to see how the govement are w...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The most popular vaccine that i know is Modern...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Of course we need to be vaccinated if we want ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message sentiment\n",
       "0   1  We need to be vaccinated to protect all person...  Positive\n",
       "1   2  it is a pleasure to see how the govement are w...  Positive\n",
       "2   3                                          Negative   Negative\n",
       "3   4  The most popular vaccine that i know is Modern...  Positive\n",
       "4   5  Of course we need to be vaccinated if we want ...  Positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the document header.\n",
    "print(f\"{Fore.MAGENTA} -------------------  overview of the dataset ------------------- \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m -------------------  The document has 1023 rows and 3 columns ------------------- \n"
     ]
    }
   ],
   "source": [
    "#count the data set\n",
    "print(f\"{Fore.MAGENTA} -------------------  The document has\", data.shape[0], \"rows and\", data.shape[1],\"columns ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m------------------- the number and percentage of missing values in the data set. ------------------- \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count  Percentage\n",
       "id             0         0.0\n",
       "message        0         0.0\n",
       "sentiment      0         0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "count =data.isnull().sum().sort_values(ascending=False)\n",
    "percentage =((data.isnull().sum()/len(data)*100)).sort_values(ascending=False)\n",
    "missing_data =pd.concat([count,percentage],axis=1,keys=['count','Percentage'])\n",
    "\n",
    "print(f\"{Fore.MAGENTA}------------------- the number and percentage of missing values in the data set. ------------------- \")\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAD8CAYAAAA42TiGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6hklEQVR4nO3dZ3gc1fn38e+tXUnucrcxxaIbiOg1NANJIJgkECCEAJEDPPSSAkQhBUECiD8JvYcETIdAAk5EDbiAaaHYLKaZIleMe7f6eV6cEVpJo77SaFe/z3Xttdpp557Z2dHcM+ecMeccIiIiIiIijWVFHYCIiIiIiPRMShZERERERCSUkgUREREREQmlZEFEREREREIpWRARERERkVBKFkREREREJJSSBRGJltlUzKLrw9nsXswcZvlJw/KDYfdGFpePI9ptE8asDLOyqMNIGz1lXxIR6SAlCyJRyLQTLn8ylPyqwGwpZu9gdjdm38Us1kVlp++2DEtUpGVm38bsX5gtwqwSs5WYfYLZPzC7ADOLICaH2dRuL7e7dfa3ZpaD2WmYlWL2ZXCcWIvZTMxuwGznTsY3Pvguiju1HBFpIB51ACKSUS4P3mPAYGAn4BTgNOAtzE7CuU8azfNToF+3RdjUb4ASYGGEMTQn6m3Ts5hdClwJVAPPAh8D2cCWwMHAccBtwfieYiGwA7A66kAiZbYd8CR+WywDXgDmATnAjsBZwAWYHY1zk6MKU0SaUrIgIqnjXHGTYWajgJuB44H/YrYnzi1Jmmded4UXyrkvgS8jjaE5UW+bnsRsLHAFsAY4AOcSjcZnAd8Garo/uBY4VwV8FHUYkfLHgBeBzYAbgEtxbmOjaUYClwFDujs8EWmZqiFJ16u7RW82BrP7MVuC2UbM3sbsJy3MdzhmT2O2LLhd/Rlm12I2OGTasuA1CLPrgr+rGtyONhuH2d+DcRVBHC9jdnbI8sYFVUTmB9N+hdlDmG0fMm19VRKzMzFLYFYezHMXZnlJ044P6qCPBcY2qrpzb9J0R2P2QFC9Yj1m64LtdUFwUhS2vbbD7ImgWsZ6zF7FbAJmE4PlTwyZZzPMbsHs82A9l2M2GbO9mvlW2s+5r4AfA1OBzYFLG8XQtF6+mWFWGKzD0mB7zsfsOcxOCKZp67as2/9G46tELcSs5uvt0VpVIL8vPInZimC7voLZd0KmKw6WMz5kXNN66z72wuDTF0mxl7W4bfzwLMzOwux/wb6xPvj77ND9o34bDA/2yboqILMx+1noerfGLC/YdxYG388HNK4G5Ledw+ylFpaTCH6ro1spcR/8HaspTRIFAOdqce45nAvbXvtg9jhmi/FVl+ZjdidmY0KmnRrEHMfsUszmBNtqPmbXYJaTNO3EpO/n4Eb7YHEwTXibhfr9bkvMzgu2Xzn++HTp19vR7HjM3gy+4yXBNu/TzLaM9rjVvD/hE4WHce4XTRIFAOeW4Ny5wCNJ5W6HWQlmb+GPAxWYzQ3i26zJ+sCU4NNljWIc32jaEzGbgj9WlmP2IWa/wyw3NHqzk/BVKjcG38H9+P9nqf59Nj1GmT0SjD+omdiOC8bfHDpeJAV0Z0G6yxDgVWAVcA++isqPgAcx2xTnrm0wtdkf8FVaVgD/AZYAOwMXAUdith/OrWlURg7wEjAUeB5/BfKLYHkTgH8AufjqCw8HMewCXALcnlT2EcA/8dUb/g18iv9H90NgAmaH4Nw7Iev4f8DhwTzPA4cA/w/YBjg0mKYsWK+fB59vSJp/ZtLfJUAt8Aa+GkNesIwbgb3wVXuSt9c4YEaw7qXAe8BWwL+Ap0NiBbPdgziHAs8F6zwcOBp4BbNjcC583vZyrhazPwHjgRMx+0XoSV29K/HVg74AHsNX4dgEv+7HA4/S9m0Jfh1fB9bh17MW+KoNkW8JvAa8D9wZxHAC8AxmP8G5R9uwjOZcjt/Wu+C/11XB8FXhkzdwP/ATYD5wN+CAY/BVcA4ATgqZZzB+H6kEHgf64Kvt/B2zWpyb1I7Yc4D/Bst8JPh8bLAe2wPnAuDcR5hNAQ7BbLsmVdDMvgl8A3gC5xa3Uuby4H0rzGI417Y7CD4Z+itQAUzGb7NtgdOB72G2bzN3cB4CDgSewR9LjsQfK0YCdQnWTPz3eBkwF7g3af6pbYoP/oz/XdQdN76P3/9zMFuBPxY8CbyMv3NyLj5paniRo2cct5oy60v98eryliYFwLmKpE8/xFdPmoL//1GJr9pY993tiXN11QefDN4LgWk03P5lSfH8DTgVWIDfXquAfYE/Aodh9m2cq06a/mL8NloJTMIfi76N/y01V7WsI7/P5o5Rt+GPOWcC00PmOyN4v6uZWEQ6zzmnl15d+wIXvB5zkJU0fEsHKxxUOtgqafghwfSvOhjcaFkTg3HXNxpeFgz/r4P+jcYNd7A6KOfgkPg2S/p7iIOVDpY52LHRdDs5WOfgnUbD7w3Knudgi6ThcQfTg3F7h8Rb1sI22zpkWJaDScHy9mk07sVg+NmNhn83aftPbBTbpw7Km2wTGONgoYMvHeS26ztueZpcB1XBtFsmDZ/aZF5Y7mCBg34hyxnezm1Zt/73OYiHjK/7/vKThuUnzXdto+n3DNZjpYNBScOLg+nHh5RRt7x7Wy274fiwbXNiMM87DgYkDe/v4K1g3E+a2QZ3O4glDd/RQbWDD9r0Pddvb+fglQb7Bwx18Fkw7qCk4ccFw/7cwrb/dhvK7Z9U9nQHpwa/yVgL82wX/O4/dbBpo3GHOqhx8K/QbQ5vOxjaqPxPg3lGh2zfqc3E0Np3X9YgNhjs/PFnvYOlDnZo9Bv6wEGFg5FJw3vGcSt8/Q8MlrWgXfP5eTd1Yccg+E7wPdzeaPj4oKziZpZX9//jnw76NhpX9/u9MGnYVsFvfamDzZOGm4OHXdhxr3O/z+aOUe87f6xufOzb0kGtgxnt3rZ66dWOl6ohSXepAX6Nc7VfD3HuC+Am/JWw5CvlFwTv/w/nVjVYinP34q9khV2ZAfgVzq1vNKwQGATcjnPTmszh3IKkTz/FXy29DOc+aDTdbPwVyt0w2zGk7CtIvkLpr07dE3zau5l4wzn3WciwWvyVW/BXAj2zzfFXAD/FX/1OnucZ/BXgxiYAWwM3N9kmzi3CX0kbDRzWrrhb4q8Y1l0dHtGGOaoIq3/u3LIOlF4JXETyFcO2WY2vJ59c/lvAg/j95JgOxNJZpwbvRTi37uuhfr//dfDp9JD5NgC/JPmKvN/HZwA7YDawnXH8huSrwM6twF+dhfor7+Cv+C4CJjao5uGrE/4I+IzwfbQhv37fx//+DwT+hr/jsxazaZidE1KN5Gz88eVC6q9A1y3vJfydhu81s+6/DtYpufwH8dV392w13rb7Y4PY/DFvMr5h++0492HSuAr8XbUcfEPhOj3juBVuk+B9QYtThXFuIQ3vNNQNfx6YTfJxsG0uxDd+P5WmVaH+iD8+Jf9v+Qm+BsbNODc/qXwHFBHePqajv8+WjlG34++KFzYafgZgND7ui6RY69WQwrsguxfnyvB1fvMbjZuKc1ODOoLjG40rw7l78XWDJzZZal3jSJWZXmW2zbwgOWhsKv4W/m5Jw/bDnygej9nxIfPkACMwG4Zzy5OGl+Or3zS2b/D+TBvi3C9436WZbbVd8L4D8EGjcW+FTF/3D6Z9jfbMhgEX46s+bAX0bzTFpkl/7xq8v9YgGav3CvCtRsPq1nNsM+u5bfC+A81VY+qYuvrsrpXpHgTOB2Zj9g98tYLXcK6jPcqUkdyouu3ewbm1IcOn4v9x74avmtCddsdXUZgaMm4a/gRmt5Bxc2hadQ/q99HBQNi6hqnGVwtprC6m+vKdq8bsbuAP+KpKDwVjTgH6AncFJ1+tc+49/EnvnvjqMnvgf98HBa8zguo2K4M56vbzgwlvhzMSX6VnO+DtRuNS93tuWVg5i4L3xjFBfa9dyXX2e8ZxK1xbf/Mhc5rhT94n4qvrDcF/X3Uq27GsfsEylgE/J7yH3QoaJmF1+/ErTaZ0bi5m82n6P7Ojv8+WjlH34aujnQH8BQCzbPx2WYmvqtmy3nI+ojI7V2ZzWr39UH97LPk1Phg3NWRccTCuOGTc1GDc+NDlqsz0LLNt+9BrzYwbF4yfkjSsqpk4G7/GJs1T5mBuM2W8EExf0IZYX2hj2YVJ8zRflaS52+It3c731RA+D+Z7w8FtDv4UfAc3BMPvTZr+5GDYtc0s76xg/MSkYX9t43pe1o7v2LUyTZ+k7zY/aXhYVZuYgwsdzEqKpcrBUw62afO2rI9tWgvjW6qG9HAz8xwRjL8naVh3VUOqdrC0hfVZ7KA2ZBtMbfP6t/w9ljn4soXv2Dn4otHwTYPvb1rSsITz1WlGtKnclmPa28GHQdk3JA2f08b9/OAWt3n9uLpqLBPbsX3b/923vC81jaEnHLea/27qqiHN78D3en0w7yIHDzi4Jtg2xa6uSlpb4q7fB9uyjVzSPP8Nhu3UTHyvp/D32fwxyk9zezDdIcHnuup917dxW/aO8xGV2bkym3m1fmfBueYfcOPc+BbGFQPFzYybSv3VBpWZ7mW2zahmhtf1gJJ8xXg1kIVzQ9tZhmtm+KrgfVOgaS8qDdXFsQv+KmYUTsc3rL2cxl2Rmu2Hv5WerO5qcXPbOGx43Xr+gO7r0/wA/N3Mr3CurMUpfVWZG4Eb8V0qHoDvUel4YCfMdiKsekILS+xQxO3bb+vu6oQdVwd3sPwwq4GhmGXju+WsZxbHN1IPu4OQSsObaWQctl3AuYWY/Rs4BrMd8FeIvwE8inNLOx2Nc29idh6+OtOhSWPq4sgj/K5KpugJx63mvIW/Yr8ZZtvj3Mdtmsv/7i/AVzX7Jo3v8Jmd2M446rbRuzi3exvnST62zg4Z39yxtSO/z9aOUbfjG3ufiW/w3b6Gzb3lfERldq7MZqjNgnSXLQjvmnJ88P5u0rDXgSGY7ZSisl8P3r/bjmkPTFHZzamh4e30ZNsE70+EjDs4ZFjdttsvtFs+f6LdWHetp+fj+m3w6aGWJm3Cd6n4T5z7Eb63q63xJ5p1WtqWnbV7M/XZxwfvyfvtyuB985Dpm6vjXney3Z7438Ufu8O6UjwoWFZYrzepFAe+GTJ8fPD+bsi424L3M6g/0UllXeu6k8nkf5LdtZ/X0nX7YFv0hONWON824P7g0+9bnb6+3clW+P38+ZBEYbNgfFh8hMbo2w/Mxl9saOuFqLr9uOkx1D/3I+y33jW/T58EzsAn3Pvgq5ZOJ7lNi0gXUbIg3SUGXNPgZNZsS/yVo2rggaRprw/e/0p4P+j9Mdu3yfDmTcJfyTmbsL6qG/bXfQ/+TsRlmDVt3Of7zx7fjrKbsxzf7qJvyLiy4L1hOWa74bsTbcg3vJuKTzLObDTPETRtrwDwFL5h6bmYHRkaodl+QT3fzvFXCB/Br8884KpWps/F7DCsUaViX0e37p/8hqQxLW3LzsrD17VPjmNPfD3q1fiuaeu8Gbz/LLiCWDf95k2WUa+uzc0W7Yjp78H71Q2+H/93SfDpb+1YXkdd3ajB8lDgd8Gne0KmfxH4BN/W40fAJzg3pc2lme2N73e+6ffs9426xqPJ3Uvegm//dD3+CcKN58vBLBUn2MsJP3HsLj3huNWS3+EbOJ+Ef1ZO2Hc4HLOb8HcQof44eABmsaTpBuAbbIfdwWvt93Qdvs3b3wl/Xs8QfJfSdR7C/386P/gd101nwNWEJ05d+fu8PYj/CXxSfEcHlyPSLnrOgnSX9/APVXobs+fxJ2En4KtnXEJy7z/OvYhZEf5gPAezp/H97Q/APxToYHyDsyPaVLJzy/APf3scmILZM0E8g/DPbtgcX+0HnFuO2XH4k8DXMXsRfzWqFv8PaD9gGL6P+s54Ef/MgGcxm46/TT8L5/6Nb8x2MXADZocAc/ANjo/C9799QsjyzsVfdbotOPmve87CsfjE4AfUV5MB56ow+yH++QqlmL2K72VmQ7A99grm34SGJ+Ytq29UlYX/bnfCX5XLwZ9Mn0TrvRn1xVcnKcPsDXz/9X3wfZvvAExudDWtpW3ZWdOB04MreTOof85CFnBmg6otzr0RlH8Q8Cb+QWSjgO/ht3PYyeSL+O/6r5g9ju9jfRXO3dJsRM49hNkP8CfcszF7El+F4Wj8fvwYzj3YiXVuiy/xvbO8j9lkfI9Dx+G3z20417Q/eOccZnfgT9ig/XcVxuBPim/B7BV8Q93yoMwj8FWgPiW59yr/nIdT8SdwszF7Fp+wZON/zwcCS4Fx7YylsReBHwdVrd7Gn2BOD90OXaFnHLdaiu8rzA7D94x1EVCI2Qv4iwd1PTuNx+9TRwfzLMbsEXzyMDPp/8a38d/7TOo7d6jzMb4B+I8xqwyW74D7cW4uzv0dsz2Ac4DPMHsumGYo/rdzEH4fOyuI4TP8M3+uAmZh9ij1z1kYCszC/w9JXteu/H3+A38xbVN8Q+1/dnA5Iu3T7gZHeunV3lddoxzff/8DDpY432f0O65xf9MN5zvA+WczLHK+r/SlDmY6uM7Bno2mbb3hne9v/D7nnyFQ6eArB9McnBEybb6DW5xvIFnuYI2Djxzc7+DoRtN2pKFgf+cbrC1wvkGccw0bLe/oYHKwrdY73+f76a65xpJ+nnHO9x++KpjnNQcTHFwUzHN0yDwjHZQ434/3Buf7Y5/j4HHnG0437fO7+e84+VXhfJ/vbzvfmPoIl/yMjYbzNmxQCtkOLnHwjPN9wJcH3/3rzjfWzmnntvT7X/Oxt9TA+V4HOzjfsHplsI1mODi8mWUNDtZ3SbAN3ndwRivf2y+db5xbEUxT1uy2qR+e5eAc5/tt3xC83nZwbuh2bmkbdKyBc5mDPAe3Br+nimAdLnBgLcw7xPn+8csdDGvncWSg833Y3+PgvWD/qnb+WS2vOihyMLCZeQuC9ZwbxLoi+G7udHBoi/tjw3HNNXAe6eAh548pNa5hI8Wub+DccL+N7rjV+neY4+A0B087/xyXSgdrnW/sfpNr3AkF9HNwpat/Jsz8YJ8b1sJvYy/nnzuz2vlnEDTdjnCUg/8Ev9NK5xsdv+l8RxLjQpZ5ioN3Xf2x6AHn/5+972BVl/4+m057fTB9eIcWeunVBS9zzkWdr0imM3PANDrQqEZSwOxBfH/h42hr40KRruCrwkwBHsC5U1qeWKQHMxuEf8LyTJzbr7XJU1juVPwdkO1xbk63lSu9mtosiGQCXyd5dMjww/DVZj5QoiA9wCXBe/PVrER6ErMRQZuY5GFx/PMO+tCw3VJXx7I3vhruc0oUpDupzYJIZsgB5mM2BfgIX2d6J3zd2kp8mwaR7mdWgG9vswe+R7L/4Nwb0QYl0mbHAldg9l/8w+qG4q/sb4dvN3Fzl0dgdja+ncLP8O1QLuvyMkWSKFkQyQxV+J4xDsU3JO+HbwD3D6AE596NMDbp3fbANxBdg98fz4k2HJF2eQPfocZB+Ebi4DvcuBK4Bt81bFf7Nf6J3Z8Dp+Dcm61ML5JSarMgIiIiIiKh1GZBRERERERCKVkQEREREZFQShZERERERCSUkgUREREREQmlZEFEREREREIpWRARERERkVBKFkREREREJJSSBRERERERCaVkQUREREREQilZEBERERGRUEoWREREREQklJIFEREREREJpWRBRERERERCKVkQEREREZFQShZERERERCSUkgUREREREQmlZEFERHo8M3Nm9pekzxeZWXEXlHNpo8+vproMEZF0omRBRETSQQXwQzMb3sXlNEgWnHPf7OLyRER6NCULIiKSDqqBu4BfNB5hZiPM7Akz+1/w2j9p+Atm9o6Z3Wlmc+uSDTN70szeNrPZZnZGMKwE6GtmM83swWDYuuD9UTM7MqnMe83sWDOLmdm1QbnvmdmZXb4lRES6kTnnoo5BRESkRcFJ+xjgPWAX4P8BA5xzxWb2EHCbc+4VM9sCeM45t4OZ3QIsdM5dbWZHAM8AI5xzy8xsqHNuhZn1Bf4HHOycW25m65xzA5LLdc4NMLNjgKOdc4VmlgN8BmwHnAKMdM79ycxygRnA8c65L7pt44iIdKF41AGIiIi0hXNujZndB1wAbEwa9S1gRzOr+zzIzAYCBwDHBPM+a2Yrk+a5IEgAADYHtgWWt1D8M8BNQUJwBDDdObfRzL4D7GxmxwXT5QXLUrIgIhlByYKIiKSTG4B3gHuShmUB+znnkhMILCl7aDR8PD7B2M85t8HMpgJ9WirUOVceTHc4cALwcN3igPOdc8+1cz1ERNKCkgURkQyXX1S6CbAVsDWQDwzDXwGvew0O3nOAGnz7gORXBbAM+CrptQRYDHwOzC0rmdAtdVqDqkOPAacBfw8GPw+cB1wLYGa7OudmAq8APwKuCe4ADAmmzwNWBonCOGDfpCKqzCzbOVcVUvwjwOnAnsDEYNhzwNlm9pJzrsrMtsNXfVqfmjUWEYmW2iyIiGSI/KLSscAewauA+uSgXxcXvR74CPgg6fVuWcmE+akqILktgZmNwlfz+b+gzcJw4FZgB/xFsOnOubPMbCT+DsAQYBr+jsCWwSKfBDYFPgZGAMXOualmdg3wfeAd59xJjcrNxidIk51zPwuGZQF/Ar6Hv8uwFN+2YXWq1l1EJEpKFkRE0lB+UWkWsBcwHvgm/ur4yChjCjEXmB68Xi4rmfBxdxYetC+occ5Vm9l+wO3OuV27MwYRkXSnZEFEJE3kF5UOxTeuPRJfd76rnzmQal8BU4CngNKykglru7IwM9sWeAzfpqESOMc597+uLFNEJNMoWRAR6cHyi0o3BU4CfgDsA8SijShlKoD/Av8EniormdBST0QiIhIRJQsiIj1MflFpX3yXn4X4Xnsy/QGaNcBU4F7g8bKSCeWRRiMiIl9TsiAi0kPkF5Xug3/Y2PHAoIjDicpK4AHg9rKSCR9GHYyISG+nZEFEJEL5RaWG733nYmD/iMPpaV4CbgImd1fXrCIi0pCSBRGRCOQXleYCpwC/AsZFHE5P9z7wR3wVpdqogxER6U2ULIiIdKP8otIc4Bzg18DoiMNJNx/gn2nwqJIGEZHuoWRBRKQbBNWNTgCuov7BYNIxHwHFZSUTHo06EBGRTKdkQUSki+UXlR4EXAvsHXUsGWY6cH5ZyYT3og5ERCRTKVkQEeki+UWlY4Eb8c9IkK5RA9wO/KGsZMLKqIMREck0ShZERFIsqHJ0DlACDIg4nN5iGfAb4G/qOUlEJHWULIiIpFB+Uel2wN3AgVHH0ku9BEwsK5kwP+pAREQygZIFEZEUyC8qjeG7Qb0c6BNxOL3dKuDcspIJD0UdiIhIulOyICLSSflFpWOAR4EDoo5FGngEOEdtGUREOk7JgohIJ+QXlR4GPASMjDoWCbUQOKmsZMK0qAMREUlHShZERDogv6g0C/gdcBmQFXE40rJq4JdlJRNujjoQEZF0o2RBRKSd8otKhwEPAodHHYu0y9/w1ZIqow5ERCRdKFkQEWmH/KLSbYBnga2jjkU65DXgh2UlExZHHYiISDrQrXMRkTbKLyrdG3gVJQrpbD/grfyi0t2jDkREJB0oWRARaYP8otKjgCnAiKhjkU7bFJiSX1SqZ2GIiLRCyYKISCvyi0rPAJ4E+kUciqTOIOC5/KLSI6IORESkJ1OyICLSgvyi0l8DdwKxqGORlOsLPJVfVHpc1IGIiPRUShZERJoRJAolUcchXSoHeCS/qPRnUQciItITKVkQEQnx5O+/ewa4q6OOQ7pFDPhbflHpyVEHIiLS06jrVBGRxorzzgDunFm79ctHV15xAJhFHZJ0i2rgB2UlE56OOhARkZ5CyYKISLLivBOAhwjuvH5Yu8UrR1Ze9U1Hlu7E9g4bgG+XlUx4NepARER6AiULIiJ1ivOOxPd6lJ08+NPaMa8eXnnN3jXE4pHEJd1tJXBgWcmE2VEHIiISNSULIiIAxXk7AG8CA8JGz60d+fphlX/eo5p4dth4yTgLgW+WlUyYF3UgIiJRUrIgIlKcNxCfKIxrabKFbtibh1Rct0sl2bndE5hE7B1g/7KSCeVRByIiEhXVwRURgXtoJVEA2NSW7/1K7oWJPlRs7IaYJHq7A7dHHYSISJSULIhI71acdzFwbFsnH2mr9pyRe8FHfanY0IVRSc8xMb+o9OyogxARiYqqIYlI71WcNx74Lx14OvMq13/W/hU3bbWevgNTHpf0NJXA+LKSCa9FHYiISHdTsiAivVNx3qb4OukjO7qIta7v7P0rbtxsDQPyUhdYz+CqK1n80K9x1VVQW0u/7fdn8IEnsfSpa6hasQCA2vL1ZPXpz5if3dxg3qrlC1g6+ZqvP1evWszgA05m0F4/YNX0+9nw6RtgRqzfYIYd+XPiA4dRvuADVjx/GxbLZvj3LyZ7yBhqy9ex9KlrGPmjK7DoH3WxCNi9rGTCV1EHIiLSnZQsiEjvU5yXDUwD9uvsota73A/3r7hp9CoGDul8YD2Hcw5XVU5WTl9cTTWLH7yEoYedQe6m9U07Vrx0N1m5/Rm8/4nNL6e2hgW3FbLJKdcRzxtJbcUGsnL7AbDmrclULZ/HsMPPY8m/rmTIwROpXr2EjV+8zdBDT2fFS3fTb5t96LNFQZevbxs9XVYyYULUQYiIdCe1WRCR3uhKUpAoAPS3ih1eyz1/6TBWL0vF8noKMyMrpy8ArrYaamsaPMjaOceGj16h/w4Htbic8rmzyB68CfE8fwOnLlEAcFXlgF+mZcVx1ZW46gosK07Vyi+pWbu8JyUKAEfmF5WeEXUQIiLdScmCiPQuxXm7Ar9M5SL7WuV2M3IvWD2KFUtSudyoudoaFt1zPgtuPpk++buSO2b7r8dVLJhNrP9gsodu2uIy1n84nX6NEoqV0+9jwW0TWf/BVAYfeDIAefsez/Jnb2HNW08xcPejWDX9vq/H9TDX5ReVbhV1ECIi3UXVkESk9yjOywJeBfbpisVXunjZIRV/yV3IiE26YvlRqS1fx5J/XcnQb51Jzoh8AJY/dyvZQzZh0N4/bHY+V1PFglsLGXParcT6N62ltfq1x3DVVQw+8KQGw8vnv8+GT15j4G5HsurlB7CsGEMOPS10GRF5CfhWWckE/QMVkYynOwsi0pucSRclCgA5Vp0/NfeXVVvYVwu6qowoZPUZQJ/NC9j4+TuAv+Ow4ZPX6Deu5SpIGz9/m5xRWzd7kt9/x/Fs+GRGg2HOOVa/+ih5+5/IqhkPMfiAn9B/p0NY8/a/U7MyqXEooOpIItIrKFkQkd6hOG8UcFVXF5NtNVu8lPMr28oWze3qsrpSzYbV1JavA6C2qoLyuTPJHrYZAOVl/u/4oOEtLmP9B9OatGmoWrHw6783fPoG2UM3azjP+y/Sd+s9ifUZgKuqAMsCM/93z/J/+UWlHe5JS0QkXcSjDkBEpJtcBwzujoLiVrvpCzkXLz6y8uovPnZbbNkdZaZazboVLCu9HlwtuFr6jTuQftvsDfh2CI2TgOq1y1n+7E2MOv5yAGqryikvm8mwI85rMN2qaZN816uWRXzQCIYefu7X42qryln3/ouM+tEfARi019Es/ddVWCzO8O9f0pWr2xGDgD+hOwwikuHUZkFEMl9x3reAF7q72BpnS79f+afVs92W23R32dItavHPXpgVdSAiIl1F1ZBEJLMV5+UCt0VRdMzciH/n/G7Ibjbn4yjKly6XBdwQdRAiIl1JyYKIZLpfANtGVXiWuWFP5Fw2ah/74IOoYpAuNT6/qPSYqIMQEekqqoYkIpmrOK8fMBdouSVuN3CONT+tKpr7cu3OPeopY5ISnwE7lpVMqIw6EBGRVNOdBRHJZGfQAxIFADMG3ZddsuVhWW/PjDoWSbmtgcKogxAR6Qq6syAimak4Lwf4HGj5EcPdzDk2nFt14UdP1+6ze9SxSEp9CowrK5lQE3UgIiKppDsLIpKpCulhiQKAGf1uzb5xx2OyXv5f1LFISm0DHB91ECIiqaZkQUQyT3FeDPh11GE0x4w+12XfvsuJsRffiDoWSanfRB2AiEiqKVkQkWaZmTOzvyR9vsjMiju4rMFmdk4H5y0zs/a0PTgBX4+8xzIj56r43/b4WeyZ16KORVJm5/yi0glRByEikkpKFkSkJRXAD9t5ot6cwUBosmBmsRQs3yvOM9LkCq8Z8T/E79/7rNjkGVHHIilTFHUAIiKppGRBRFpSDdyFf1ZBA2Y2wsyeMLP/Ba/9g+HFZnZR0nTvm1k+UAJsbWYzzexaMxtvZlPM7CEgEUz7pJm9bWazzeyMDsb8PeAbHZy325kR+3X8kf1+Hn/85ahjkZQ4IL+odKeogxARSRUlCyLSmluBk8wsr9HwG4HrnXN7AccCd7eynCLgM+fcrs65i4NhewO/dc7tGHw+1Tm3B7AncIGZDetAvGd3YJ5ImZF1YeyfB/wm/uD0qGORlDg96gBERFJFyYKItMg5twa4D7ig0ahvAbeY2UxgMjDIzAa2c/FvOue+SPp8gZnNAl4HNqe9T14uztsE+HY7Y+gRzLAz46UHXRG/Z1rUsUinnZJfVJoTdRAiIqmgZEFE2uIG4DSgf9KwLGC/4E7Brs65TZ1za/FVl5KPLX1aWO76uj/MbDw+AdnPObcL8G4r84Y5GUhd+4cI/DT+wsH/F79jatRxSKcMA34QdRAiIqmgZEFEWuWcWwE8hk8Y6jwPnFf3wcx2Df4sA3YPhu0ObBkMXwu0dOchD1jpnNtgZuOAfTsQakY8RfdH8enjb8m+SXcY0puqIolIRlCyICJt9RcguVekC4A9zew9M/sAOCsY/gQwNKiedDbwCYBzbjkwI2jwfG3I8p8F4mb2HvBHfFWktivO2xXImIalR8VeP/iv2X+eGnUc0mHfyi8q3TzqIEREOsucc1HHICLSecV5V5EmXaa2xys135h2ctWlB0cdh3TIBWUlE26OOggRkc7QnQURyRTHRR1AVzgg9v7B/8gpng66spOGvh91ACIinaU7CyKS/orzdgFmRh1GV5pVu9XLR1desb8jSxd50kcVMKKsZMLqqAMREeko/dMRkUyQkXcVku2S9fmBz+T85tUsamuijkXaLBv4btRBiIh0hpIFEckEh0UdQHcYlzX/gBdyLn4jRk111LFIm6kqkoikNVVDEpH0VpzXB1gN9JqHYM2rHfn6oZV/3qOaeHbUsUirVuGrIinBE5G0pDsLIpLu9qYXJQoAW2Qt2Xd67s/fzaGqIupYpFWDgV2iDkJEpKOULIhIuts/FQs59amNjLx2Ld+4bd3Xw37/Ujk7376OXe9Yx3fuX8+itbVN5vt4WQ273rHu69egq9dww+v+HH7m4hr2vXs9u96xjj3vWsebC31zgxnzqtn59nXs9dd1fLrCL3NVuePwB9bT1ru9Y2zF3q/kXvB+Hyo2dnbdpct9M+oAREQ6SsmCiKS7A1KxkIm7ZvPsyf0aDLt4/1zeO3sAM88awFHbxbliWtML+dsPjzHzLD/N22f0p1+2ccw4XzvokhfKuezgHGaeNYArDsnlkhfKAfjLa5U88aO+XHVoH27/XyUAf5xWwaUH5GJmbY55pK3eY0buBR/1o3x9R9dbuoWSBRFJW0oWRCR9FecZKToRO2hsnKF9G56oD8qt/7y+Elo7jX/xixq2HprF2MH+0GoGa4L8YnU5jBnol5Adg43VsKHKkR2Dz1bUsnBtLQfnx9sd9zBbu9trued9NoANa9o9s3QXJQsikrbUwFlE0ldxXgHwXqoWV7aqlqMe2sD75wz4ethvXyznvveqyMs1phT2Y0T/5q+xnPrURnbfJMZ5e/smFB8ureHwBzbggFoHr57an7GDs5i5uIaz/lNO32y4/5i+XPR8OX88JJdth8U6HPta13f2/hU3braGAXkdXoh0pc3KSiYsjDoIEZH20p0FEUlnKamC1JIrD+vD/F8M5KSCbG55s7LZ6SprHJM/rub4HevvDtz+VhXXH+7nv/7wPpw22Tcv2HV0jNdP78+Uwv58vrKWMQOzcMAJj2/g5H9u5Kt1TdtGtGagbdzptdzzFw1hzYp2zyzdQXcXRCQtKVkQkXSWksbNbfGTgmye+LD53i+fmVPN7ptkMWpA/WF10qxKfriDTx6O3zH+dQPnOs45/jS9gt8flMvl0yq4fHwuJ++czU1vNJ+UtKS/Vezwau4Fy4ezammHFiBdadeoAxAR6QglCyKSznbqyoXPWV5/cj/542rGDW/+kPnw+1Wc+I2Gjz0YMzCLaXP9Ml76ooZthzWcf9KsKiZsG2dIX2NDFWSZf22o6njMfa1y2xm5F64ZzYqvOr4U6QLbRx2AiEhHtL81nYhIzzE2VQs68YkNTC2rYdkGx2bXreXy8bk8/Wk1Hy+rJctg7OAs7pjQB4BFa2s5fXI5T5/ke0/aUOV44fMa7jyqb4Nl/vV7fbjw2XKqa6FPHO5KGr+hyjFpVhXPBz0w/XLfHI59bCM5MXj42IbLaa9cq9p6eu7Pyw6p+MuXCxmxSacWJqmyXdQBiIh0hBo4i0h6Ks4bCKgHoBZUudj8b1Vea3Pd6M2ijkXYCPQvK5mgf7oiklZUDUlE0lXK7ipkqmyr2fzFnIuytraFc6OORegLbBF1ECIi7aVkQUTSVX7UAaSDuNWOeT7nkj7jbN7nUcciqookIulHyYKIpCvdWWijmLlRT+f8ZuDO9tmcqGPp5baOOgARkfZSsiAi6UrJQjtkmRvxZM4fhu1un3wUdSy92PCoAxARaS8lCyKSrpQstFOWuaGP5xSP3jdr9uyoY+mlhkUdgIhIeylZEJF0pWShA7KMwQ9nX7n5QVmz3os6ll5IyYKIpB0lCyKSrkZFHUC6MmPQpOxrtvp21lszo46ll1GyICJpR8mCiKSr7NYnkeaYMeCu7Ou2PyrrtbejjqUXUZsFEUk7ShZEJF3Fog4g3ZnR9+bsm79xbNb0/0UdSy+hOwsiknaULIhIulKykAJm5P45+45dfhL77+tRx9IL9Ik6ABGR9lKyICLpKh51AJnCjJwr43/f89TYM69GHUuGU4IrImlHyYKIpCudeKWQGfHfx+/f55zYUzOijiWDaZ8VkbSjK3Mikq50/GqnSqhcEo8tWxSPr5wfj6+fnx2vWBCP1yyKx2PLYrGcNbGsAeX2av4AXlsRdayZyVbBhKiDEBFpF/2zFZF0pau0gdVZWau/jMeWLYjH187Pjq+fH49XL4zH+Soei6+MxfqtNxtYaTbMwWDMxgBjWlqedVPcvY+rjDoCEZH2UrIgIukqo5OFKqhaGo8tWxiPr1wQj6+b1/QuQP+NZkNqYBhmeUBe1DFLq6qjDkBEpL2ULIhIukrLNldrsmz14lh8xYLs+Or58fiG+dnxqgXxOEvisfiKrFifdVk2qNJsqIOhmG0CbBJ1zJIyShZEJO0oWRCRdLUaGBJ1EADVUL00FrQFyI6vmx+Pl8/Pjtd+GY/bslgsZ3VW1oCNWZZXDcN1F6BX2xB1ACIi7aVkQUTS1XK6OFlYa7b2q3h82YJ4fM387Pj6ednx6oXxeO3ieCx7pb8LMLCi/i7AaGB0V8YjaW9x1AGIiLSXkgURSVfLgG3aO1MN1CyLxZYtisdWzs+Or50fz66Ynx2vXhSPZS2LxXJWZcX6bcyywcFdgIHAwNSHLr3Ul1EHICLSXkoWRCRdLUv+sN5s3eJ4bPnCeHzVvOz4hvnx7MoF2XG+isViy2OxvsFdgCHONwgeBYyKKG7pvZQsiEjaUbIgImnplE1GJT7Pjm+xISurri3AAGAAMDbq2ESaoWpIIpJ2lCyISFqa2Se3Ctg56jhE2kF3FkQk7aRl14MiIsCCqAMQaSclCyKSdpQsiEi6mh91ACLtpGpIIpJ2lCyISLoqizoAkXZaGHUAIiLtpWRBRNLVHGBj1EGItFFZojCxJuogRETaS8mCiKSlRGGiBngv6jhE2ujdqAMQEekIJQsiks50Aibp4p2oAxAR6QglCyKSznQCJulC+6qIpCUlCyKSznRnQdKF9lURSUtKFkQknSWA6qiDEGnF4kRhQs9YEJG0pGRBRNJWojBRAXwQdRwirdBdBRFJW0oWRCTdvRV1ACKt0D4qImlLyYKIpLsXog5ApBXPRx2AiEhHKVkQkXT3HGq3ID3XCuC1qIMQEekoJQsiktYShYmVwKtRxyHSjOeCBwiKiKQlJQsikglKow5ApBnaN0UkrSlZEJFM8J+oAxAJUQM8E3UQIiKdoWRBRNJeojDxAfBF1HGINPJ6ojCxIuogREQ6Q8mCiGQKVfeQnkb7pIikPSULIpIpJkcdgEgj2idFJO0pWRCRTPEiMC/qIEQCbyYKE7OjDkJEpLOULIhIRkgUJmqBu6OOQyRwZ9QBiIikgpIFEckkf0MPaJPorQEeiToIEZFUULIgIhkjUZhYhBqVSvQeTBQmNkQdhIhIKihZEJFMo+ofEjXtgyKSMZQsiEimeQ6YG3UQ0mu9mShMzIo6CBGRVFGyICIZRQ2dJWK6qyAiGUXJgohkor8CqjMu3e0r1LBZRDKMkgURyTiJwsRXwK1RxyG9ztVq2CwimUbJgohkqmuAtVEHIb3GAuCOqIMQEUk1JQsikpEShYnlwA1RxyG9xpWJwkRF1EGIiKSakgURyWR/AVZGHYRkvC/wDwQUEck4ShZEJGMlChOrgT9HHYdkvCsShYmqqIMQEekKShZEJNPdCCyNOgjJWB8D90cdhIhIV1GyICIZLVGYWA/8Keo4JGP9PlGYqIk6CBGRrqJkQUR6g1uBt6IOQjJOaaIw8Y+ogxAR6UpKFkQk4wVXfk8HqqOORTLGOuDsqIMQEelqShZEpFdIFCZmocbOkjqXJgoT86MOQkSkqylZEJHe5HJgTtRBSNp7DT0hXER6CXPORR2DiEi3KZhUMB54CbCIQ+lyrtbxWfFnZA/JZuwvxrJx7kYWTVqEq3IQgzE/HUO/rfq1aV6g2fnXz1nPokmLyMrOYrOzNiN3VC4162uYf/t8xv5qLGYZtakrgd0ShYkPog5ERKQ76M6CiPQqicLEVHrJA7SWP7+c3DG5X39e/NhiRh49km3+uA2jjhnF4kcXt3neluZf/uxytjhvC0YdO4oVL60AYMnkJYw4akSmJQoAVytREJHeRMmCiPRGFwHzog6iK1WtqGLtrLUMOWjI18PMjNqNtQDUbKwhe0h2m+dtcf4YuCpHbWUtFjMqllRQvbKa/uP6d8GaRWoWcFXUQYiIdCdVQxKRXqlgUsGewCtAbmvTpqN5t8xjxFEjqNlYw/JnlzP2F2MpX1TO3D/PxeGgFrb63VbkDM9p07xAs/PXVU/KysliszM2Y/Ejixn5w5Hkjs6oTbsK2DNRmPgs6kBERLqT7iyISK+UKEy8BZwXdRxdYc3MNcQHxemb37fB8BUvrWD0iaMZd904NvnJJiz8+8I2z9vS/H3H9mXrP2zNlkVbUrm0kviQOADzbpvH/DvnU7067XusdcDJShREpDdSsiAivVaiMHE3cHfUcaTahjkbWPPuGj7+1ccsuH0B6z5cx/w757NqxioG7TkIgEF7DWLj5xvbPC/Q6vzOOZZMXsLI749kyZNLGHX0KAbvN5jlLyzv4jXuclckChOlUQchIhKFeNQBiIhE7DxgF2CvqANJldHHj2b08aMBWPfhOpY/u5zNz9ycOb+Zw/qP1jNghwGs/3A9OaOaVkFqbl6A7MHZLc6/6pVVDNxlILH+MWora/3lqCz83+nraXyXuyIivZKSBRHp1RKFiYqCSQXHAW8Dw6OOpyuN+dkYvnzwS6gFyzY2/dmmAFStrGLhPQvJ/2V+h+YHqK2oZdWMVeRf5Jcx/PDhzLtlHhYzNj97865apa72Ob76kRr3iUivpQbOIiJAwaSCw4DngFjUsUiPsBHYL3jyt4hIr6U2CyIiQKIw8SJwBr4xq/RuNcCJShRERJQsiIh8LVGY+Dvw86jjkEg54PREYeKpqAMREekJlCyIiCRJFCZuAn4bdRwSmYsThYl7ow5CRKSnULIgItJIojBxFXB11HFIt7siUZj4S9RBiIj0JGrgLCLSjIJJBTcB50cdh3SLKxOFid9FHYSISE+jOwsiIs27ELgn6iCky12jREFEJJySBRGRZgT9658O3Bp1LNIlHPDbRGGiKOpARER6KlVDEhFpg4JJBZcCV0Ydh6RMBTAxUZh4JOpARER6MiULIiJtVDCpYCJwF5AdcSjSOcuBoxOFiVeiDkREpKdTsiAi0g4FkwoOAZ4AhkQdi3TIp8CRicLEnKgDERFJB2qzICLSDonCxBRgH+CTqGORdnsV2E+JgohI2ylZEBFpp+Bkcx/g6ahjkTa7HzgsUZhYFnUgIiLpRNWQREQ6oWBSwXnAtUCfqGORUKuBcxKFiYeiDkREJB0pWRAR6aSCSQU7AQ8BO0cdizTwCnByojAxN+pARETSlaohiYh0UqIwMRvYG7ge33e/RKsa+D0wXomCiEjn6M6CiEgKFUwq+A5wL7BJxKH0Vp8CJyUKE29GHYiISCbQnQURkRRKFCaex1dHugfdZehOVcANwG5KFEREUkd3FkREukjBpII9gRuBb0YdS4Z7BvhFojDxcdSBiIhkGiULIiJdrGBSwYnANcDmUceSYT4GfpkoTKgLWxGRLqJkQUSkGxRMKugHXBK8+kYcTrpbBVwB3JIoTFRFHIuISEZTsiAi0o0KJhVsDvwBOAXIjTicdLMO+CtwdaIwsTTqYEREegMlCyIiESiYVDAaOB84GxgScTg93ZfATcAdicLEqohjERHpVZQsiIhEqGBSQX/gNOAXQH600fQ4HwJ/Bh5IFCYqow5GRKQ3UrIgItIDFEwqiAHHA78E9oo4nKhNwycJpYnChP5JiYhESMmCiEgPUzCpYAfgx8CJwLYRh9NdZgGPAI8kChNlEcciIiIBJQsiIj1YwaSCPfCJwwlkXternwIPAw8nChMfRh2MiIg0pWRBRCQNFEwqMGB/4DjgEKAAsEiDar8a4F1gCvBYojDxVsTxiIhIK5QsiIikoYJJBUOAA4GDgtfuQCzSoJqqBP4HTMe3Q3g1UZhYG21IIiLSHkoWREQyQMGkggH4Ow/fBLbHt3XYFhjYTSGsBOYEr4+AV4DXE4WJ8m4qX0REuoCSBRGRDFYwqWAU9YnDtsDW+Oc6DAQGJL0PIPwhcZXAenwysCp4LaE+MZgDzEkUJpZ34WqIiEhElCyIiAgABZMKsvFJgwEbgXJ1XSoi0rspWRARERERkVBZUQcgIiIiIiI9k5IFEREREREJpWRBRETaxMxqzGymmb1vZv8ws37tnH+MmT0e/L2rmR2ZNO77ZlaU6phFRKRz1GZBRETaxMzWOecGBH8/CLztnLuug8uaCOzpnDsvhSGKiEiK6c6CiIh0xMvANmY21MyeNLP3zOx1M9sZwMwODu5CzDSzd81soJnlB3clcoArgBOC8SeY2UQzu8XM8syszMyyguX0M7P5ZpZtZlub2bNm9raZvWxm4yJcfxGRXkHJgoiItIuZxYHvAgngcuBd59zOwKXAfcFkFwHnOud2xT9pemPd/M65SuAPwKPOuV2dc48mjVsNzAIODgZ9D3jOOVcF3AWc75zbI1j+bV22kiIiAkA86gBERCRt9DWzmcHfLwN/A94AjgVwzr1kZsPMLA+YAVwXVFf6p3NugZm1tZxHgROAKcCPgdvMbAD+6dT/SFpO2EPkREQkhZQsiIhIW20M7hR8zcIzAOecKzGzUuBI4HUz+xZQ3sZyJgNXm9lQYA/gJaA/sKpx+SIi0rVUDUlERDpjOnASgJmNB5Y559aY2dbOuYRz7hrgLaBx+4K1wMCwBTrn1gFvAjcC/3HO1Tjn1gBfmNnxQVlmZrt0xQqJiEg9JQsiItIZxcCeZvYeUAIUBsN/HjRmnoVvr/BMo/mmADvWNXAOWe6jwMnBe52TgNOCZc4GfpC61RARkTDqOlVERERERELpzoKIiIiIiIRSsiAiIiIiIqGULIiIiIiISCglCyIiIiIiEkrJgoiIiIiIhFKyICIiIiIioZQsiIiIiIhIKCULIiIiIiISSsmCiIiIiIiEUrIgIiIiIiKhlCyIiIiIiEgoJQsiIiIiIhJKyYKIiIiIiIRSsiAiIiIiIqGULIiIiIiISCglCyIiIiIiEur/AyvS96d/CWUuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check the Distributin of Sentiments\n",
    "category = ['Negative','Neutral','Positive']\n",
    "values = [data.sentiment.str.count(\"Negative\").sum(),data.sentiment.str.count(\"Neutral\").sum(),data.sentiment.str.count(\"Positive\").sum()]\n",
    "plt.pie(values, labels= category,autopct ='%0.2f%%')\n",
    "plt.title('------------------- percentage Distribution by Sentiment Category -------------------',fontsize=20,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- Special characters and punctuation cleaning operations completed. ------------------- \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will now clean the data by removing the special characters.\n",
    "\n",
    "\"\"\"\n",
    "#set up punctuations we want to be replaced\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})|(\\=)|(\\#)|(\\ยง)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s/><br\\s/?)|(-)|(/)|(:).\")\n",
    "print(f\"{Fore.BLUE}------------------- Special characters and punctuation cleaning operations completed. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- preparation of the cleaning functions completed. ------------------- \n"
     ]
    }
   ],
   "source": [
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "  tempArr = []\n",
    "  for line in df:\n",
    "    # send to tweet_processor\n",
    "    tmpL = p.clean(line)\n",
    "    # remove puctuation\n",
    "    tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "    tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "    tempArr.append(tmpL)\n",
    "  return tempArr\n",
    "\n",
    "print(f\"{Fore.BLUE}------------------- preparation of the cleaning functions completed. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- The message cleaning operation is complete. ------------------- \n"
     ]
    }
   ],
   "source": [
    "# Cleaning up tweets\n",
    "clean_tweet = clean_tweets(data[\"message\"])\n",
    "print(f\"{Fore.BLUE}------------------- The message cleaning operation is complete. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- The clean data column has been successfully added to the dataset. ------------------- \n"
     ]
    }
   ],
   "source": [
    "# append cleaned tweets to the training data\n",
    "clean_tweet = pd.DataFrame(clean_tweet)\n",
    "data[\"clean\"] = clean_tweet\n",
    "print(f\"{Fore.BLUE}------------------- The clean data column has been successfully added to the dataset. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- overview of the dataset with the clean_data column. ------------------- \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>We need to be vaccinated to protect all person...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>we need to be vaccinated to protect all person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>it is a pleasure to see how the govement are w...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>it is a pleasure to see how the govement are w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The most popular vaccine that i know is Modern...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>the most popular vaccine that i know is modern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Of course we need to be vaccinated if we want ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>of course we need to be vaccinated if we want ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message sentiment  \\\n",
       "0   1  We need to be vaccinated to protect all person...  Positive   \n",
       "1   2  it is a pleasure to see how the govement are w...  Positive   \n",
       "2   3                                          Negative   Negative   \n",
       "3   4  The most popular vaccine that i know is Modern...  Positive   \n",
       "4   5  Of course we need to be vaccinated if we want ...  Positive   \n",
       "\n",
       "                                               clean  \n",
       "0  we need to be vaccinated to protect all person...  \n",
       "1  it is a pleasure to see how the govement are w...  \n",
       "2                                           negative  \n",
       "3  the most popular vaccine that i know is modern...  \n",
       "4  of course we need to be vaccinated if we want ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the cleaned and uncleaned tweets\n",
    "print(f\"{Fore.BLUE}------------------- overview of the dataset with the clean_data column. ------------------- \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function who help to count unique words\n",
    "\n",
    "def wordCount(text):\n",
    "    count = Counter()\n",
    "    for i in text.values:\n",
    "        for word in i.split():\n",
    "            count[word]+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- The column 'clean' contains:  2540  unique words ------------------- \n"
     ]
    }
   ],
   "source": [
    "# count the number of unique words contained in the set of cleaned expressions. \n",
    "text= data.clean\n",
    "counter = wordCount(text)\n",
    "print(f\"{Fore.BLUE}------------------- The column 'clean' contains: \",len(counter),\" unique words ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive [0 0 1]\n",
      "Positive [0 0 1]\n",
      "Negative [1 0 0]\n",
      "Positive [0 0 1]\n",
      "Positive [0 0 1]\n",
      "Neutral [0 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sentiment into numeric value\n",
    "y = pd.get_dummies(data['sentiment']).values\n",
    "[print(data['sentiment'][i], y[i]) for i in range(0,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- The data was successfully split. ------------------- \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, we will divide the file (the 'clean' column) \n",
    "into two parts: one part for training and one part for testing.\n",
    "\n",
    "X_train ==> train_sentences\n",
    "X_test ==> test_sentences\n",
    "y_train ==> train_labels\n",
    "y_test ==> test_labels\n",
    "\n",
    "\"\"\"\n",
    "X = data['clean']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f\"{Fore.GREEN}------------------- The data was successfully split. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- Operations completed. ------------------- \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will now define the maximum length of a sentence in terms of the number of words it can contain.\n",
    "But first, we define the word count as the number of unique words.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# define the number of words.\n",
    "num_words = len(counter)\n",
    "# maximum number of words in a sentence.\n",
    "max_length = 42\n",
    "print(f\"{Fore.GREEN}------------------- Operations completed. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- tokenization process complete. ------------------- \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will now tokenize the text ('clean' column):\n",
    "This means that a unique number is associated with each unique word in the text.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, split=\" \")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "print(f\"{Fore.GREEN}------------------- tokenization process complete. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- visualization of the result obtained after tokenization. ------------------- \n",
      "\u001b[30m {'the': 1, 'to': 2, 'i': 3, 'vaccine': 4, 'is': 5, 'not': 6, 'and': 7, 'of': 8, 'a': 9, 'it': 10, 'vaccinated': 11, 'in': 12, 'covid': 13, 'vaccines': 14, 'for': 15, 'are': 16, 'that': 17, 'be': 18, 'get': 19, 'will': 20, 'this': 21, 'do': 22, 'am': 23, 'my': 24, 'have': 25, 'so': 26, 'we': 27, 'you': 28, 'all': 29, 'as': 30, 'no': 31, 'with': 32, '19': 33, 'people': 34, 'but': 35, 'effects': 36, 'us': 37, 'first': 38, 'at': 39, 'if': 40, 'side': 41, 'who': 42, 'pfizer': 43, 'on': 44, 'more': 45, 'they': 46, 'vaccination': 47, 'me': 48, 'want': 49, 'or': 50, 'about': 51, 'there': 52, 'take': 53, 'good': 54, 'was': 55, 'think': 56, 'has': 57, 'against': 58, 'can': 59, 'these': 60, 'because': 61, 'one': 62, 'today': 63, 'been': 64, 'getting': 65, 'dose': 66, 'by': 67, 'their': 68, 'why': 69, 'just': 70, 'virus': 71, 'our': 72, 'when': 73, 'would': 74, 'like': 75, 'should': 76, 'very': 77, 'after': 78, 'know': 79, 'what': 80, 'doses': 81, 'got': 82, 'does': 83, 'any': 84, 'go': 85, 'from': 86, 'going': 87, 'without': 88, 'how': 89, 'than': 90, 'those': 91, 'everyone': 92, 'now': 93, 'make': 94, 'let': 95, 'lives': 96, 'only': 97, 'many': 98, 'an': 99, 'out': 100, 'free': 101, 'your': 102, 'well': 103, 'need': 104, 'received': 105, 'being': 106, 'vaccinate': 107, 'still': 108, 'them': 109, 'had': 110, 'two': 111, 'health': 112, 'stop': 113, 'never': 114, 'time': 115, 'which': 116, 'some': 117, 'see': 118, 'day': 119, 'coronavirus': 120, 'other': 121, 'were': 122, 'did': 123, 'moderna': 124, 'trust': 125, 'even': 126, 'up': 127, 'thing': 128, 'thank': 129, 'world': 130, 'really': 131, 'save': 132, 'believe': 133, 'pandemic': 134, 'disease': 135, 'news': 136, 'also': 137, 'life': 138, 'safe': 139, 'effective': 140, 'much': 141, 'case': 142, 'way': 143, 'say': 144, 'nothing': 145, 'sure': 146, 'wait': 147, 'far': 148, 'risk': 149, 'yes': 150, 'live': 151, 'thanks': 152, 'companies': 153, 'protect': 154, 'its': 155, 'done': 156, 'maybe': 157, 'long': 158, 'cannot': 159, 'biontech': 160, 'bad': 161, 'most': 162, 'important': 163, 'soon': 164, 'possible': 165, 'prevent': 166, 'caign': 167, 'corona': 168, 'happy': 169, 'children': 170, 'new': 171, 'better': 172, 'money': 173, 'enough': 174, 'choice': 175, 'year': 176, 'shot': 177, 'yet': 178, 'problem': 179, 'end': 180, 'years': 181, 'receive': 182, 'afraid': 183, 'prices': 184, 'injection': 185, 'before': 186, 'anyone': 187, 'family': 188, 'best': 189, 'great': 190, 'others': 191, 'must': 192, 'use': 193, 'find': 194, 'then': 195, 'since': 196, 'whether': 197, 'again': 198, 'please': 199, 'care': 200, 'point': 201, 'ones': 202, 'same': 203, 'astrazeneca': 204, 'severe': 205, 'reactions': 206, 'allergic': 207, 'said': 208, 'last': 209, 'too': 210, 'term': 211, 'days': 212, 'useful': 213, 'able': 214, 'real': 215, 'pharmaceutical': 216, 'hope': 217, 'finally': 218, 'serious': 219, 'business': 220, 'personally': 221, 'myself': 222, 'quickly': 223, 'feeling': 224, 'necessary': 225, 'testing': 226, 'consequences': 227, 'makes': 228, 'week': 229, 'developed': 230, 'science': 231, 'order': 232, 'negative': 233, 'already': 234, 'second': 235, 'solution': 236, 'might': 237, 'less': 238, 'into': 239, 'his': 240, 'covid19': 241, 'few': 242, 'vaccinations': 243, 'die': 244, 'st': 245, 'population': 246, 'propaganda': 247, 'yourself': 248, 'person': 249, 'months': 250, 'mrna': 251, 'least': 252, 'ive': 253, 'taken': 254, 'effectiveness': 255, 'immunity': 256, 'emergency': 257, 'fear': 258, 'hours': 259, 'friends': 260, 'country': 261, 'approves': 262, 'cure': 263, 'injected': 264, 'own': 265, 'safety': 266, 'authorities': 267, 'food': 268, 'working': 269, 'laboratories': 270, 'both': 271, 'old': 272, 'im': 273, 'back': 274, 'million': 275, 'lot': 276, 'patients': 277, 'big': 278, 'course': 279, 'made': 280, 'right': 281, 'grateful': 282, 'immune': 283, 'normal': 284, 'help': 285, 'mind': 286, 'here': 287, 'supermarket': 288, 'remain': 289, 'cases': 290, 'young': 291, 'public': 292, 'scientists': 293, 'decision': 294, 'alone': 295, 'according': 296, 'rather': 297, 'hospitals': 298, 'using': 299, 'stay': 300, 'consumer': 301, 'approved': 302, 'interested': 303, 'vaccin': 304, 'variant': 305, 'found': 306, 'turn': 307, 'next': 308, 'part': 309, 'future': 310, 'wonder': 311, 'such': 312, 'come': 313, 'later': 314, 'tests': 315, 'situation': 316, 'infected': 317, 'given': 318, 'africa': 319, 'countries': 320, 'give': 321, 'feel': 322, 'avoid': 323, 'administration': 324, 'form': 325, 'risks': 326, 'anti': 327, 'ever': 328, 'fine': 329, 'develop': 330, 'exle': 331, 'government': 332, 'almost': 333, 'could': 334, 'ready': 335, 'work': 336, 'question': 337, 'rid': 338, 'share': 339, 'panic': 340, 'charge': 341, 'media': 342, 'reaction': 343, 'reduce': 344, 'thats': 345, 'ago': 346, 'nobody': 347, 'confinement': 348, 'ahead': 349, 'pfizers': 350, 'fighting': 351, 'taking': 352, 'available': 353, 'encourage': 354, 'through': 355, 'idea': 356, 'refuse': 357, 'frankly': 358, 'over': 359, 'arrived': 360, 'says': 361, 'continue': 362, 't': 363, 'nurse': 364, 'home': 365, 'around': 366, 'healthcare': 367, 'administering': 368, 'company': 369, 'self': 370, 'mass': 371, 'difference': 372, 'anyway': 373, 'decide': 374, 'between': 375, 'wear': 376, 'doubt': 377, 'certainly': 378, 'variants': 379, 'shows': 380, 'adverse': 381, 'except': 382, 'weeks': 383, 'down': 384, 'gives': 385, 'dying': 386, 'likely': 387, 'yesterday': 388, 'roll': 389, 'johnson': 390, 'announced': 391, 'human': 392, 'offering': 393, 'arrive': 394, 'night': 395, 'research': 396, 'information': 397, 'fake': 398, 'rate': 399, 'known': 400, 'change': 401, 'everything': 402, 'gets': 403, 'update': 404, 'g': 405, 'system': 406, 'guinea': 407, 'favour': 408, 'force': 409, 'remember': 410, 'works': 411, 'employees': 412, 'kiss': 413, 'contamination': 414, 'millions': 415, 'pharma': 416, 'body': 417, 'him': 418, 'set': 419, 'follow': 420, 'excited': 421, 'social': 422, 'diseases': 423, 'conspiracy': 424, 'wants': 425, 'prefer': 426, 'every': 427, 'stock': 428, 'mean': 429, 'opportunity': 430, 'another': 431, 'tested': 432, 'reason': 433, 'five': 434, 'control': 435, 'look': 436, 'leave': 437, 'data': 438, 'workers': 439, 'cost': 440, 'view': 441, 'trump': 442, 'doctors': 443, 'six': 444, 'vaccinating': 445, 'longer': 446, 'morning': 447, 'causing': 448, 'ingredients': 449, 'quite': 450, 'heres': 451, 'whole': 452, 'natural': 453, 'humanity': 454, 'off': 455, 'scammers': 456, 'coming': 457, 'post': 458, 'democrats': 459, 'while': 460, 'production': 461, 'kind': 462, 'press': 463, 'happened': 464, 'administered': 465, 'fact': 466, 'citizen': 467, 'begin': 468, 'russian': 469, 'rights': 470, 'supply': 471, 'gates': 472, 'friday': 473, 'biontechs': 474, 'injections': 475, 'allowed': 476, 'keep': 477, 'suffer': 478, 'actually': 479, 'experimental': 480, 'month': 481, 'fight': 482, 'true': 483, 'effect': 484, 'pleasure': 485, 'approve': 486, 'stable': 487, 'hear': 488, 'matter': 489, 'happens': 490, 'breaking': 491, 'trials': 492, 'latest': 493, 'cancer': 494, 'otherwise': 495, 'thinking': 496, 'test': 497, 'frightening': 498, 'protein': 499, 'response': 500, 'honestly': 501, 'politicians': 502, 'tired': 503, 'especially': 504, 'careful': 505, 'fda': 506, 'stand': 507, 'shame': 508, 'light': 509, 'away': 510, 'suspicious': 511, 'bosses': 512, 'compulsory': 513, 'create': 514, 'where': 515, 'danger': 516, 'talk': 517, 'rollout': 518, 'support': 519, 'drugs': 520, 'show': 521, 'reliable': 522, 'approval': 523, 'rush': 524, 'allow': 525, 'developing': 526, 'herd': 527, 'choose': 528, 'thankful': 529, 'number': 530, 'concern': 531, 'harmful': 532, 'useless': 533, 'passport': 534, 'preserve': 535, 'someone': 536, 'dont': 537, 'spread': 538, 'pretty': 539, 'option': 540, 'shown': 541, 'unexpected': 542, 'impossible': 543, 'clear': 544, 'products': 545, 'false': 546, 'little': 547, 'told': 548, 'opinion': 549, 'things': 550, 'including': 551, 'affraid': 552, 'deaths': 553, 'start': 554, 'industry': 555, 'forms': 556, 'receiving': 557, 'barrier': 558, 'gestures': 559, 'technology': 560, 'affected': 561, 'difficult': 562, 'positive': 563, 'china': 564, 'afterwards': 565, 'dr': 566, 'he': 567, 'problems': 568, 'dubai': 569, 'residents': 570, 'may': 571, 'fed': 572, 'pharmacists': 573, 'essential': 574, 'regulator': 575, 'loved': 576, 'three': 577, 'took': 578, 'via': 579, 'something': 580, 'making': 581, 'until': 582, 'called': 583, 'hesitation': 584, 'etc': 585, 'lose': 586, 'doing': 587, 'sick': 588, 'story': 589, 'word': 590, 'damn': 591, 'bit': 592, 'mother': 593, 'ourselves': 594, 'drug': 595, 'whats': 596, 'hospital': 597, 'death': 598, 'zero': 599, 'wary': 600, 'governments': 601, 'anything': 602, 'pseudo': 603, 'usa': 604, 'saves': 605, 'americans': 606, 'scams': 607, 'comes': 608, 'travel': 609, 'respect': 610, 'assure': 611, 'responsible': 612, 'saturated': 613, 'allergy': 614, 'though': 615, 'pessimistic': 616, 'starting': 617, 'once': 618, 'proud': 619, 'hearing': 620, 'imperative': 621, 'authorizes': 622, 'pregnant': 623, 'women': 624, 'mask': 625, 'don': 626, 'direct': 627, 'raise': 628, 'price': 629, 'theres': 630, 'london': 631, 'linked': 632, 'staff': 633, 'started': 634, 'mine': 635, 'wednesday': 636, 'respecting': 637, 'russia': 638, 'empty': 639, 'apparently': 640, 'multiple': 641, 'probably': 642, 'close': 643, 'unnecessary': 644, 'controlled': 645, 'listen': 646, 'together': 647, 'knows': 648, 'requires': 649, 'dangerous': 650, 'rodents': 651, 'experiencing': 652, 'freedom': 653, 'low': 654, 'harm': 655, 'advised': 656, 'created': 657, 'hand': 658, 'sanitizer': 659, 'bacterial': 660, 'resistant': 661, 'leading': 662, 'worse': 663, 'treated': 664, 'wondering': 665, 'consent': 666, 'united': 667, 'leaky': 668, 'transmission': 669, 'shed': 670, 'deadly': 671, 'refused': 672, 'imagine': 673, 'medical': 674, 'easier': 675, 'strong': 676, 'special': 677, 'arm': 678, 'speed': 679, 'confirmed': 680, 'county': 681, 'area': 682, 'stores': 683, 'reports': 684, 'each': 685, 'employer': 686, 'flexible': 687, 'broken': 688, 'large': 689, 'scale': 690, 'moreover': 691, 'debate': 692, 'teach': 693, 'cells': 694, 'trigger': 695, 'putting': 696, 'european': 697, 'eu': 698, 'unvaccinated': 699, 'warning': 700, 'treatment': 701, 'senior': 702, 'measure': 703, 'barier': 704, 'return': 705, 'costs': 706, 'commercial': 707, 'operation': 708, 'tunnel': 709, 'distributed': 710, 'mandatory': 711, 'holiday': 712, 'christmas': 713, 'always': 714, 'bar': 715, 'particular': 716, 'pity': 717, 'scientific': 718, 'everybody': 719, 'friend': 720, 'fully': 721, 'invented': 722, 'contribute': 723, 'moderne': 724, 'prevention': 725, 'definitely': 726, 'amazing': 727, 'feels': 728, 'moment': 729, 'daughter': 730, 'community': 731, 'awareness': 732, 'fast': 733, 'nice': 734, 'hard': 735, 'quality': 736, 'antibodies': 737, 'full': 738, 'object': 739, 'inspire': 740, 'sheep': 741, 'pig': 742, 'despite': 743, 'super': 744, 'keeping': 745, 'realy': 746, 'review': 747, 'europe': 748, 'december': 749, 'vaxxers': 750, 'increasing': 751, 'older': 752, 'trying': 753, 'step': 754, 'house': 755, 'contact': 756, 'advise': 757, 'member': 758, 'evidence': 759, 'substance': 760, 'therefore': 761, 'expected': 762, 'name': 763, 'suggests': 764, 'predict': 765, 'ensure': 766, 'affordable': 767, 'glad': 768, 'design': 769, 'incentives': 770, 'market': 771, 'ventilators': 772, 'economists': 773, 'navigate': 774, 'cause': 775, 'adolescents': 776, 'common': 777, 'during': 778, 'stepping': 779, 'talking': 780, 'beginning': 781, 'none': 782, 'speaking': 783, 'uk': 784, 'beat': 785, 'ways': 786, 'deplore': 787, 'deeply': 788, 'hostile': 789, 'seems': 790, 'per': 791, 'interesting': 792, 'helps': 793, 'remains': 794, 'compared': 795, 'parents': 796, 'third': 797, 'goes': 798, 'distancing': 799, 'happening': 800, 'conditions': 801, 'disadvantages': 802, 'childrens': 803, 'bill': 804, 'hell': 805, 'cool': 806, 'cant': 807, 'historic': 808, 'honest': 809, 'stopped': 810, 'profit': 811, 'smallpox': 812, 'local': 813, 'site': 814, 'asked': 815, 'experience': 816, 'nd': 817, 'confident': 818, 'based': 819, 'global': 820, 'west': 821, 'ppe': 822, 'chain': 823, 'major': 824, 'producing': 825, 'woman': 826, 'gonna': 827, 'illness': 828, 'anyways': 829, 'simply': 830, 'means': 831, 'easing': 832, 'concerns': 833, 'themselves': 834, 'teenagers': 835, 'june': 836, 'angry': 837, 'importance': 838, 'popular': 839, 'newly': 840, 'humans': 841, 'success': 842, 'knew': 843, 'evil': 844, 'basic': 845, 'needs': 846, 'sufficiently': 847, 'offer': 848, 'record': 849, 'program': 850, 'socially': 851, 'waiting': 852, 'advance': 853, 'worries': 854, 'specialy': 855, 'batch': 856, 'seen': 857, 'across': 858, 'door': 859, 'claiming': 860, 'purposes': 861, 'hopefully': 862, 'prevents': 863, 'hospitalized': 864, 'contagious': 865, 'strange': 866, 'product': 867, 'early': 868, 'lucky': 869, 'questions': 870, 'capitalism': 871, 'matters': 872, 'confidence': 873, 'peoples': 874, 'participate': 875, 'treatments': 876, 'understand': 877, 'produce': 878, 'credible': 879, 'development': 880, 'receives': 881, 'decided': 882, 'excellent': 883, 'read': 884, 'authorized': 885, 'run': 886, 'lockdown': 887, 'hesitate': 888, 'looking': 889, 'stocks': 890, 'intend': 891, 'drop': 892, 'agree': 893, 'task': 894, 'announces': 895, 'lie': 896, 'pure': 897, 'report': 898, 'programme': 899, 'regarding': 900, 'taxpayers': 901, 'african': 902, 'potential': 903, 'animals': 904, 'bullshit': 905, 'react': 906, 'saying': 907, 'usually': 908, 'calling': 909, 'shape': 910, 'traditional': 911, 'measures': 912, 'lets': 913, 'chinese': 914, 'joe': 915, 'biden': 916, 'seem': 917, 'present': 918, 'proven': 919, 'gov': 920, 'containment': 921, 'majority': 922, 'advice': 923, 'neighbours': 924, 'type': 925, 'man': 926, 'biggest': 927, 'hot': 928, 'rest': 929, 'patents': 930, 'damage': 931, 'scary': 932, 'forever': 933, 'certainty': 934, 'protects': 935, 'reliability': 936, 'hurry': 937, 'judgment': 938, 'jennifer': 939, 'haller': 940, 'private': 941, 'tetanus': 942, 'inject': 943, 'transmitting': 944, 'wont': 945, 'hesitating': 946, 'changed': 947, 'room': 948, 'pigs': 949, 'worst': 950, 'wiser': 951, 'resuscitation': 952, 'thermally': 953, 'wish': 954, 'impact': 955, 'past': 956, 'toll': 957, 'aware': 958, 'enjoy': 959, 'protection': 960, 'wanted': 961, 'amazed': 962, 'turnaround': 963, 'suppose': 964, 'documents': 965, 'id': 966, 'vacation': 967, 'polyethylene': 968, 'glycol': 969, 'wrong': 970, 'team': 971, 'acquired': 972, 'infection': 973, 'capable': 974, 'decimating': 975, 'act': 976, 'licensed': 977, 'complaining': 978, 'wearing': 979, 'piss': 980, 'surrounding': 981, 'aka': 982, 'eager': 983, 'drama': 984, 'shipments': 985, 'ship': 986, 'missouris': 987, 'nation': 988, 'senate': 989, 'republicans': 990, 'investors': 991, 'surreal': 992, 'dropping': 993, 'becerra': 994, 'medicine': 995, 'cdc': 996, 'dizzy': 997, 'faints': 998, 'tennessee': 999, 'hospita': 1000, 'quiet': 1001, 'professor': 1002, 'eyeing': 1003, 'urges': 1004, 'canadians': 1005, 'british': 1006, 'airways': 1007, 'suspend': 1008, 'wanting': 1009, 'encouraging': 1010, 'massive': 1011, 'novel': 1012, 'minister': 1013, 'investigating': 1014, 'sheet': 1015, 'providers': 1016, 'founder': 1017, 'biotechnology': 1018, 'hat': 1019, 'attention': 1020, 'evening': 1021, 'trusts': 1022, 'republic': 1023, 'plans': 1024, 'inoculation': 1025, 'starts': 1026, 'hopes': 1027, 'launch': 1028, 'watchdog': 1029, 'rospotrebnadzor': 1030, 'noway': 1031, 'chipped': 1032, 'responsibility': 1033, 'somewhere': 1034, 'priority': 1035, 'queue': 1036, 'faint': 1037, 'began': 1038, 'jabs': 1039, 'wouldnt': 1040, 'fair': 1041, 'equitable': 1042, 'disparity': 1043, 'phew': 1044, 'patented': 1045, 're': 1046, 'several': 1047, 'tinkered': 1048, 'marketed': 1049, 'listened': 1050, 'gp': 1051, 'morons': 1052, 'misinformation': 1053, 'believing': 1054, 'cobbled': 1055, 'guaranteed': 1056, 'pandemie': 1057, 'complicated': 1058, 'unless': 1059, 'eat': 1060, 'similar': 1061, 'call': 1062, 'thag': 1063, 'statistics': 1064, 'percentage': 1065, 'possibility': 1066, '4': 1067, 'prediction': 1068, 'sudden': 1069, 'overuse': 1070, 'antibacterial': 1071, 'lead': 1072, 'mutations': 1073, 'bacteria': 1074, 'reassured': 1075, 'warned': 1076, 'encouraged': 1077, 'volunteer': 1078, 'nights': 1079, 'shift': 1080, 'efforts': 1081, 'kingdom': 1082, 'scotland': 1083, 'place': 1084, 'inconvenients': 1085, 'poison': 1086, 'anymore': 1087, 'iran': 1088, 'helpfully': 1089, 'tp': 1090, 'hoarding': 1091, 'organisation': 1092, 'agns': 1093, 'buzyn': 1094, 'laugh': 1095, 'loud': 1096, 'september': 1097, 'deal': 1098, 'chemo': 1099, 'alzheimers': 1100, 'parkinsons': 1101, 'aids': 1102, 'miracle': 1103, 'australia': 1104, 'outbreak': 1105, 'woolworths': 1106, 'shopping': 1107, 'elderly': 1108, 'disabled': 1109, 'needle': 1110, 'inserted': 1111, 'regionals': 1112, 'came': 1113, 'sullivan': 1114, 'flocked': 1115, 'purchase': 1116, 'cleaning': 1117, 'supplies': 1118, 'toilet': 1119, 'paper': 1120, 'goods': 1121, 'altruism': 1122, 'imposed': 1123, 'fundamental': 1124, 'oregon': 1125, 'corporations': 1126, 'fund': 1127, 'pressure': 1128, 'commission': 1129, 'premiere': 1130, 'short': 1131, 'period': 1132, 'complain': 1133, 'saturate': 1134, 'conflicting': 1135, 'officials': 1136, 'medicare': 1137, 'claims': 1138, 'identity': 1139, 'theft': 1140, 'schemes': 1141, 'packages': 1142, 'ap': 1143, 'respected': 1144, 'astra': 1145, 'astrazeneka': 1146, 'controversy': 1147, 'survival': 1148, 'clearly': 1149, 'formally': 1150, 'casts': 1151, 'votes': 1152, 'throwing': 1153, 'laughing': 1154, 'ass': 1155, 'thought': 1156, 'schools': 1157, 'closed': 1158, 'carriers': 1159, 'level': 1160, 'practically': 1161, 'disappeared': 1162, 'require': 1163, 'specific': 1164, 'consideration': 1165, 'hundred': 1166, 'altruists': 1167, '21': 1168, 'fewer': 1169, 'naysayers': 1170, 'race': 1171, 'inflated': 1172, 'council': 1173, 'communicate': 1174, 'warn': 1175, 'society': 1176, 'reputation': 1177, 'premature': 1178, 'total': 1179, 'capitalists': 1180, 'charade': 1181, 'dunno': 1182, 'trustworthy': 1183, 'sfr': 1184, 'spoke': 1185, 'dull': 1186, 'achey': 1187, 'sore': 1188, 'deltoid': 1189, 'privileged': 1190, 'supervising': 1191, 'torn': 1192, 'firstish': 1193, 'historical': 1194, 'congratulate': 1195, 'vloggers': 1196, 'promotion': 1197, 'promote': 1198, 'unfortunate': 1199, 'normally': 1200, 'study': 1201, 'spends': 1202, 'bothered': 1203, 'wrap': 1204, 'process': 1205, 'favor': 1206, 'current': 1207, 'attacks': 1208, 'definition': 1209, 'vocation': 1210, 'adapt': 1211, 'mutating': 1212, 'armed': 1213, 'tonight': 1214, 'flexing': 1215, 'forcing': 1216, 'denial': 1217, 'preventive': 1218, 'curative': 1219, 'infancy': 1220, 'rare': 1221, 'm': 1222, 'meeting': 1223, 'possibly': 1224, 'approving': 1225, 'bumped': 1226, 'wonders': 1227, 'certificate': 1228, 'fly': 1229, 'cruise': 1230, 'subway': 1231, 'concert': 1232, 'school': 1233, 'nor': 1234, 'hopeful': 1235, 'gain': 1236, 'alarmed': 1237, 'mainly': 1238, 'massively': 1239, 'observe': 1240, 'brought': 1241, 'perfection': 1242, 'plan': 1243, 'questionable': 1244, 'project': 1245, 'sue': 1246, 'often': 1247, 'regularly': 1248, 'office': 1249, 'prioritized': 1250, 'constantly': 1251, 'poorly': 1252, 'researched': 1253, 'realise': 1254, 'medicin': 1255, 'foreseen': 1256, 'challenge': 1257, 'accessible': 1258, 'grandmother': 1259, 'march': 1260, 'contain': 1261, 'govement': 1262, 'gene': 1263, 'therapy': 1264, 'reassure': 1265, 'harmlessness': 1266, 'senators': 1267, 'insider': 1268, 'trading': 1269, 'doubles': 1270, 'capitol': 1271, 'hill': 1272, 'folk': 1273, 'caught': 1274, 'protected': 1275, 'excuse': 1276, 'emmanuel': 1277, 'macron': 1278, 'himself': 1279, 'images': 1280, 'broadcast': 1281, 'television': 1282, 'pain': 1283, 'shoulder': 1284, 'sister': 1285, 'law': 1286, 'sim': 1287, 'trouble': 1288, 'passing': 1289, 'f': 1290, 'reached': 1291, 'agreement': 1292, 'federal': 1293, 'courage': 1294, 'pleased': 1295, 'wonderful': 1296, 'heroes': 1297, 'slowness': 1298, 'caigns': 1299, 'caused': 1300, 'brussels': 1301, 'bureaucracy': 1302, 'move': 1303, 'types': 1304, 'harmless': 1305, 'comorbidity': 1306, 'limit': 1307, 'notbeen': 1308, 'competition': 1309, 'recipe': 1310, 'day3': 1311, 'deprogramming': 1312, 'catastrophic': 1313, 'vi': 1314, 'waking': 1315, 'played': 1316, 'role': 1317, 'alive': 1318, 'california': 1319, 'nightgov': 1320, 'gav': 1321, 'discussing': 1322, 'capacity': 1323, 'resist': 1324, 'fourth': 1325, 'fifth': 1326, 'sixth': 1327, 'seventh': 1328, 'loop': 1329, 'ends': 1330, 'canton': 1331, 'due': 1332, 'jan': 1333, 'todayimpressive': 1334, 'resource': 1335, 'mobilization': 1336, 'epidemic': 1337, 'cured': 1338, 'medication': 1339, 'phase': 1340, 'begins': 1341, 'dec': 1342, 'molecules': 1343, 'treat': 1344, 'sham': 1345, 'automatically': 1346, 'enters': 1347, 'bloodstream': 1348, 'aged': 1349, 'pre': 1350, 'existing': 1351, 'autoimmune': 1352, 'include': 1353, 'hypertension': 1354, 'diabetes': 1355, 'asthma': 1356, 'respiratory': 1357, 'liver': 1358, 'kidney': 1359, 'stabilised': 1360, 'chronic': 1361, 'tend': 1362, 'ignoramuses': 1363, 'charlatans': 1364, 'theorists': 1365, 'put': 1366, 'chip': 1367, 'catching': 1368, 'arms': 1369, 'peace': 1370, 'ontarian': 1371, 'minutes': 1372, 'p': 1373, 'kids': 1374, 'towards': 1375, 'precipitation': 1376, 'baby': 1377, 'different': 1378, 'incredible': 1379, 'singapore': 1380, 'impression': 1381, 'maximum': 1382, 'leads': 1383, 'recherches': 1384, 'experts': 1385, 'mankind': 1386, 'wreaked': 1387, 'havoc': 1388, 'centuries': 1389, 'busting': 1390, 'balls': 1391, 'hide': 1392, 'mild': 1393, 'moderate': 1394, 'everywhere': 1395, 'results': 1396, 'entire': 1397, 'paul': 1398, 'biya': 1399, 'basis': 1400, 'extreme': 1401, 'thr': 1402, 'compagnie': 1403, 'suspicion': 1404, 'mistrust': 1405, 'holding': 1406, 'deliveries': 1407, 'banning': 1408, 'exports': 1409, 'viable': 1410, 'street': 1411, 'regret': 1412, 'grandchildren': 1413, 'becoming': 1414, 'unlike': 1415, 'giving': 1416, 'jam': 1417, 'spirit': 1418, 'guide': 1419, 'notgiven': 1420, 'paranoid': 1421, 'litteraly': 1422, 'causes': 1423, 'shortage': 1424, 'legacy': 1425, 'registered': 1426, 'tol': 1427, 'appointment': 1428, 'w': 1429, 'patient': 1430, 'bion': 1431, 'hundreds': 1432, 'citizens': 1433, 'families': 1434, 'impressive': 1435, 'africans': 1436, 'highly': 1437, 'risked': 1438, 'job': 1439, 'firefighters': 1440, 'expectedcovid': 1441, 'superior': 1442, 'image': 1443, 'dromois': 1444, 'reserves': 1445, 'mid': 1446, 'july': 1447, 'complete': 1448, 'vijay': 1449, 'reddy': 1450, 'sterile': 1451, 'sad': 1452, 'rages': 1453, 'risking': 1454, 'efficacy': 1455, 'mutation': 1456, 'interact': 1457, 'writing': 1458, 'book': 1459, 'receivers': 1460, 'momentous': 1461, 'creation': 1462, 'suffering': 1463, 'atrocities': 1464, 'berlin': 1465, 'rolling': 1466, 's': 1467, 'heh': 1468, 'purely': 1469, 'write': 1470, 'pays': 1471, 'guarantee': 1472, 'non': 1473, 'poisons': 1474, 'left': 1475, 'flocking': 1476, 'tragedy': 1477, 'corporate': 1478, 'greed': 1479, 'names': 1480, 'gloves': 1481, 'volatility': 1482, 'levels': 1483, 'become': 1484, 'growing': 1485, 'mot': 1486, 'folks': 1487, 'sunday': 1488, 'eff': 1489, 'savinghuman': 1490, 'proposals': 1491, 'syringes': 1492, 'clever': 1493, 'posing': 1494, 'vulnerable': 1495, 'late': 1496, 'j': 1497, 'oripire': 1498, 'intriguing': 1499, 'outcome': 1500, 'distanced': 1501, 'patiently': 1502, 'ceo': 1503, 'firsti': 1504, 'takeanother': 1505, 'dead': 1506, 'arrives': 1507, 'oman': 1508, 'colleagues': 1509, 'reported': 1510, 'nhs': 1511, 'providing': 1512, 'rushed': 1513, 'economic': 1514, 'quarterly': 1515, 'exciting': 1516, 'january': 1517, 'ehpad': 1518, 'testify': 1519, 'partnership': 1520, 'unbelievably': 1521, 'indeed': 1522, 'clearconfirmed': 1523, 'skeptics': 1524, 'concerned': 1525, 'check': 1526, 'recap': 1527, 'alternatives': 1528, 'grandmas': 1529, 'secrets': 1530, 'bothering': 1531, 'insist': 1532, 'madness': 1533, 'incite': 1534, 'desease': 1535, 'academic': 1536, 'critical': 1537, 'selection': 1538, 'tick': 1539, 'remembers': 1540, 'childhood': 1541, 'fighter': 1542, 'planes': 1543, 'missiles': 1544, 'neonuclear': 1545, 'weapons': 1546, 'fauci': 1547, 'aner': 1548, 'defense': 1549, 'politicized': 1550, 'play': 1551, 'anyhow': 1552, 'plants': 1553, 'watching': 1554, 'smooth': 1555, 'operating': 1556, 'machine': 1557, 'prisoner': 1558, 'accept': 1559, 'traumatize': 1560, 'lost': 1561, 'refusing': 1562, 'collective': 1563, 'suicide': 1564, 'hurraah': 1565, 'manufacturers': 1566, 'expenditure': 1567, 'urge': 1568, 'however': 1569, 'willing': 1570, 'atshmatic': 1571, 'mucus': 1572, 'masks': 1573, 'choices': 1574, 'union': 1575, 'access': 1576, 'ceased': 1577, 'passed': 1578, 'hands': 1579, 'military': 1580, 'industrialists': 1581, 'main': 1582, 'objects': 1583, 'multinational': 1584, 'fuss': 1585, 'forgot': 1586, 'comparison': 1587, 'doesnt': 1588, 'worked': 1589, 'tiring': 1590, 'victimisation': 1591, 'secret': 1592, 'deceiving': 1593, 'innocent': 1594, 'marc': 1595, 'siegel': 1596, 'nyu': 1597, 'langone': 1598, 'tiniest': 1599, 'soreness': 1600, 'neither': 1601, 'produced': 1602, 'unsafe': 1603, 'worried': 1604, 'approvals': 1605, 'expects': 1606, 'shots': 1607, 'states': 1608, 'corresponds': 1609, 'approv': 1610, 'modernas': 1611, 'joins': 1612, 'battle': 1613, 'key': 1614, 'gift': 1615, 'grandmothers': 1616, 'retirement': 1617, 'annihilate': 1618, 'imminent': 1619, 'paranormal': 1620, 'girl': 1621, 'frontline': 1622, 'value': 1623, 'convince': 1624, 'relatives': 1625, 'slip': 1626, 'animal': 1627, 'helped': 1628, 'sydney': 1629, 'minimal': 1630, 'immense': 1631, 'died': 1632, 'manufactured': 1633, 'recalls': 1634, 'supposed': 1635, 'autumn': 1636, 'else': 1637, 'undergo': 1638, 'tears': 1639, 'gave': 1640, 'joy': 1641, 'liquidate': 1642, 'ideal': 1643, 'targets': 1644, 'pucking': 1645, 'wht': 1646, 'pants': 1647, 'vacated': 1648, 'ill': 1649, 'benefit': 1650, 'previous': 1651, 'department': 1652, 'justice': 1653, 'shut': 1654, 'website': 1655, 'state': 1656, 'uses': 1657, 'fall': 1658, 'flies': 1659, 'conceived': 1660, 'haste': 1661, 'aim': 1662, 'worlds': 1663, 'pension': 1664, 'explain': 1665, 'melove': 1666, 'verb': 1667, 'showed': 1668, 'alongside': 1669, 'microchip': 1670, 'u2': 1671, 'album': 1672, 'tomorrow': 1673, 'monthsall': 1674, 'hcw': 1675, 'assuage': 1676, 'anxiety': 1677, 'liberty': 1678, 'procurement': 1679, 'pay': 1680, 'r': 1681, 'd': 1682, 'manufacturing': 1683, 'makers': 1684, 'destined': 1685, 'begun': 1686, 'prototype': 1687, 'laboratory': 1688, 'siberia': 1689, 'russias': 1690, 'power': 1691, 'warm': 1692, 'spreading': 1693, 'beneficial': 1694, 'tv': 1695, 'tendency': 1696, 'europeans': 1697, 'grey': 1698, 'safest': 1699, 'proliferate': 1700, 'watch': 1701, 'victim': 1702, 'donation': 1703, 'mule': 1704, 'recruitment': 1705, 'tactics': 1706, 'vogue': 1707, 'coronavirusdo': 1708, 'fooled': 1709, 'compares': 1710, 'currently': 1711, 'exposed': 1712, 'cimas': 1713, 'recommends': 1714, 'urgently': 1715, 'needed': 1716, 'clinic': 1717, 'lots': 1718, 'tourists': 1719, 'bring': 1720, 'billions': 1721, 'usual': 1722, 'shitty': 1723, 'lucrative': 1724, 'model': 1725, 'permanent': 1726, 'delaware': 1727, 'stole': 1728, 'orcas': 1729, 'rice': 1730, 'football': 1731, 'stadium': 1732, 'sanitary': 1733, 'ridiculous': 1734, 'leaders': 1735, 'vacations': 1736, 'uae': 1737, 'timely': 1738, 'achieve': 1739, 'delta': 1740, 'distant': 1741, 'totally': 1742, 'irresponsible': 1743, 'interessed': 1744, 'painless': 1745, 'als': 1746, 'ok': 1747, 'barely': 1748, 'text': 1749, 'messenger': 1750, 'rna': 1751, 'advanced': 1752, 'list': 1753, 'try': 1754, 'graveling': 1755, 'feet': 1756, 'killed': 1757, 'k': 1758, 'appreciate': 1759, 'composition': 1760, 'tells': 1761, 'eligible': 1762, 'article': 1763, 'explaining': 1764, 'cameroon': 1765, 'crap': 1766, 'adds': 1767, 'jab': 1768, 'difficulty': 1769, 'picking': 1770, 'solved': 1771, 'operator': 1772, 'cat': 1773, 'mouse': 1774, 'game': 1775, 'obviously': 1776, 'eric': 1777, 'shawn': 1778, 'herein': 1779, 'usedas': 1780, 'influence': 1781, 'chance': 1782, 'armor': 1783, 'increase': 1784, 'restrictions': 1785, 'decrease': 1786, 'israel': 1787, 'conscious': 1788, 'meanwhile': 1789, 'significant': 1790, 'views': 1791, 'touch': 1792, 'calls': 1793, 'emails': 1794, 'visitors': 1795, 'asking': 1796, 'personal': 1797, 'usernames': 1798, 'pasords': 1799, 'links': 1800, 'kits': 1801, 'tell': 1802, 'greater': 1803, 'vary': 1804, 'pperson': 1805, 'hence': 1806, 'neutrality': 1807, 'daft': 1808, 'hurts': 1809, 'ears': 1810, 'listening': 1811, 'coffin': 1812, 'opted': 1813, 'water': 1814, 'authorization': 1815, 'developer': 1816, 'proposing': 1817, 'headlines': 1818, 'governors': 1819, 'mayors': 1820, 'grocery': 1821, 'shops': 1822, 'concessions': 1823, 'hardware': 1824, 'determining': 1825, 'arose': 1826, 'desolation': 1827, 'bottom': 1828, 'top': 1829, 'billionaires': 1830, 'naivety': 1831, 'sometimes': 1832, 'kinda': 1833, 'neutral': 1834, 'chloroquine': 1835, 'medicines': 1836, 'eyes': 1837, 'moments': 1838, 'sisters': 1839, 'lasts': 1840, 'sell': 1841, 'emirati': 1842, 'merkel': 1843, 'acceptable': 1844, 'professionals': 1845, 'nurses': 1846, 'caregivers': 1847, 'numbers': 1848, 'scare': 1849, 'serving': 1850, 'cobaille': 1851, 'bogus': 1852, 'kit': 1853, 'takes': 1854, 'heist': 1855, 'looting': 1856, 'half': 1857, 'africas': 1858, 'resources': 1859, 'recommended': 1860, 'polishes': 1861, 'decades': 1862, 'noir': 1863, 'thomasall': 1864, 'involved': 1865, 'she': 1866, 'poooor': 1867, 'listed': 1868, 'various': 1869, 'markets': 1870, 'pushing': 1871, 'stories': 1872, 'finding': 1873, 'cures': 1874, 'push': 1875, 'identified': 1876, 'undoubtably': 1877, 'regulators': 1878, 'calm': 1879, 'behaviour': 1880, 'occurred': 1881, 'tuesday': 1882, 'consider': 1883, 'starvation': 1884, 'malnutrition': 1885, 'invention': 1886, 'labs': 1887, 'arabian': 1888, 'peninsula': 1889, 'president': 1890, 'pushed': 1891, 'high': 1892, 'arses': 1893, 'thead': 1894, 'epidemiologist': 1895, 'whitout': 1896, 'confined': 1897, 'guidance': 1898, 'whilst': 1899, 'breastfeeding': 1900, 'hurt': 1901, 'rusty': 1902, 'piece': 1903, 'iron': 1904, 'financial': 1905, 'scandal': 1906, 'oh': 1907, 'freer': 1908, 'completely': 1909, 'liveplease': 1910, 'transparent': 1911, 'face': 1912, 'proper': 1913, 'bra': 1914, 'l': 1915, 'investor': 1916, 'paywall': 1917, 'dosis': 1918, 'memorial': 1919, 'mir': 1920, 'gild': 1921, 'monthly': 1922, 'higher': 1923, 'appears': 1924, 'positioned': 1925, 'nursing': 1926, 'homes': 1927, 'pennsylvania': 1928, 'prepare': 1929, 'distribut': 1930, 'allergies': 1931, 'tolerating': 1932, 'officially': 1933, 'reasonable': 1934, 'whose': 1935, 'medium': 1936, 'bosnia': 1937, 'sinovac': 1938, 'confused': 1939, 'contradictory': 1940, 'circulating': 1941, 'motivated': 1942, 'having': 1943, 'okay': 1944, 'sceptical': 1945, 'healthy': 1946, 'became': 1947, 'history': 1948, 'owe': 1949, 'her': 1950, 'debt': 1951, 'gratitude': 1952, 'bravery': 1953, 'wave': 1954, 'potentially': 1955, 'saving': 1956, 'thousands': 1957, 'pessimist': 1958, 'existence': 1959, 'astrazaneca': 1960, 'learned': 1961, 'sent': 1962, 'positiv': 1963, 'efficient': 1964, 'toxic': 1965, 'advantages': 1966, 'outweigh': 1967, 'eventually': 1968, 'god': 1969, 'naturally': 1970, 'relevant': 1971, 'expensive': 1972, 'soared': 1973, 'lying': 1974, 'winning': 1975, 'joke': 1976, 'issues': 1977, 'transporting': 1978, 'nura': 1979, 'emir': 1980, 'festi': 1981, 'brave': 1982, 'scenario': 1983, 'uncertain': 1984, 'theyd': 1985, 'buy': 1986, 'extrapolated': 1987, 'cartels': 1988, 'payback': 1989, 'directly': 1990, 'chose': 1991, 'yours': 1992, 'shall': 1993, 'secondary': 1994, 'complications': 1995, 'o': 1996, 'percent': 1997, 'saudis': 1998, 'expats': 1999, 'interest': 2000, 'economy': 2001, 'crowded': 2002, 'rots': 2003, 'opt': 2004, 'issued': 2005, 'advising': 2006, 'beware': 2007, 'fraudulent': 2008, 'rapidly': 2009, 'facilitate': 2010, 'honored': 2011, 'gettin': 2012, 'stabilitechs': 2013, 'intended': 2014, 'delivered': 2015, 'disruptive': 2016, 'capsule': 2017, 'efficacious': 2018, 'capsules': 2019, 'inexpensive': 2020, 'posted': 2021, 'hey': 2022, 'terfs': 2023, 'heard': 2024, 'trans': 2025, 'vax': 2026, 'lick': 2027, 'counters': 2028, 'transit': 2029, 'seats': 2030, 'boose': 2031, 'tough': 2032, 'times': 2033, 'myth': 2034, 'terf': 2035, 'meet': 2036, 'seriously': 2037, 'property': 2038, 'scam': 2039, 'technique': 2040, 'destroy': 2041, 'york': 2042, 'city': 2043, 'relieved': 2044, 'fo': 2045, 'menstruation': 2046, 'urgent': 2047, 'warnings': 2048, 'chaos': 2049, 'hunt': 2050, 'green': 2051, 'releases': 2052, 'alaskan': 2053, 'worker': 2054, 'suffered': 2055, 'reasons': 2056, 'stella': 2057, 'maris': 2058, 'saved': 2059, 'treats': 2060, 'leaving': 2061, 'mortality': 2062, 'contract': 2063, 'transmit': 2064, 'sosomething': 2065, 'kills': 2066, 'emotional': 2067, 'scarier': 2068, 'above': 2069, 'thesis': 2070, 'convincing': 2071, 'contradictions': 2072, 'boom': 2073, 'delt': 2074, 'affect': 2075, 'heath': 2076, 'organization': 2077, 'import': 2078, 'handled': 2079, 'bringing': 2080, 'fresh': 2081, 'air': 2082, 'peaceful': 2083, 'failed': 2084, 'deliver': 2085, 'promises': 2086, 'building': 2087, 'trial': 2088, 'extending': 2089, 'effort': 2090, 'kissed': 2091, 'goodbye': 2092, 'beautiful': 2093, 'distance': 2094, 'successfully': 2095, 'within': 2096, 'programm': 2097, 'collect': 2098, 'kuwait': 2099, 'anesthesia': 2100, 'surgeries': 2101, 'apart': 2102, 'introduced': 2103, 'b': 2104, 'dog': 2105, 'chickens': 2106, 'hole': 2107, 'decates': 2108, 'deseases': 2109, 'commodity': 2110, 'primarily': 2111, 'used': 2112, 'rd': 2113, 'th': 2114, 'polished': 2115, 'theories': 2116, 'oclock': 2117, 'mostly': 2118, 'reassuring': 2119, 'undecided': 2120, 'looked': 2121, 'nonprofits': 2122, 'medicaid': 2123, 'thankfully': 2124, 'fought': 2125, 'four': 2126, 'dread': 2127, 'further': 2128, 'incentive': 2129, 'join': 2130, 'ranks': 2131, 'age': 2132, 'limits': 2133, 'youre': 2134, 'toronto': 2135, 'ontarios': 2136, 'cold': 2137, 'doctor': 2138, 'induce': 2139, 'spike': 2140, 'carried': 2141, 'pretend': 2142, 'protocol': 2143, 'funerals': 2144, 'traumatic': 2145, 'playing': 2146, 'revisit': 2147, 'museums': 2148, 'whatever': 2149, 'immunizes': 2150, 'liable': 2151, 'arriving': 2152, 'announce': 2153, 'experiments': 2154, 'summary': 2155, 'particularly': 2156, 'helpful': 2157, 'primary': 2158, 'constraints': 2159, 'status': 2160, 'region': 2161, 'condition': 2162, 'holidays': 2163, 'bonus': 2164, 'delighted': 2165, 'forget': 2166, 'tragic': 2167, 'episode': 2168, 'century': 2169, 'recover': 2170, 'remedy': 2171, 'accepted': 2172, 'auditioned': 2173, 'visible': 2174, 'poisson': 2175, 'men': 2176, 'mama': 2177, 'michel': 2178, 'workso': 2179, 'yay': 2180, 'congratulations': 2181, 'win': 2182, 'single': 2183, 'convinced': 2184, 'victims': 2185, 'compensated': 2186, 'concerning': 2187, 'besides': 2188, 'held': 2189, 'warehouses': 2190, 'wife': 2191, 'slaves': 2192, 'idiot': 2193, 'hold': 2194, 'accountable': 2195, 'brothers': 2196, 'gone': 2197, 'deceived': 2198, 'immigrant': 2199, 'muslim': 2200, 'couple': 2201, 'pendemia': 2202, 'vehemently': 2203, 'opposed': 2204, 'partly': 2205, 'againts': 2206, 'virusbecause': 2207, 'vacination': 2208, 'persones': 2209, 'drasticaly': 2210, 'reduced': 2211, 'lethal': 2212, 'unknown': 2213, 'models': 2214}\n"
     ]
    }
   ],
   "source": [
    "# visualization of the result obtained after tokenization.\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"{Fore.GREEN}------------------- visualization of the result obtained after tokenization. ------------------- \")\n",
    "print(f\"{Fore.BLACK}\",word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Let's apply the tokenisation operation to each expression in column x. \n",
    "This will allow us to observe that each expression is identifiable by a group of numbers.\n",
    "\"\"\"\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need to be vaccinated to protect all person around us\n",
      "\u001b[34m first sentence. ===>  we need to be vaccinated to protect all person around us\n",
      "\u001b[34m first sentence. ===>  [444, 250, 346, 347, 961, 2, 18, 11, 63, 3, 23, 962, 39, 1, 963]\n",
      "\u001b[32m second sentence.  ===> it is a pleasure to see how the govement are working for our help i thing the vaccination is good for all of us\n",
      "\u001b[34m second sentence. ===>  [964, 3, 19, 1, 4, 7, 1, 41, 36, 16, 205, 42, 5, 612, 296, 2, 1, 965, 10, 5, 48, 26, 966, 297, 6, 19, 1, 4]\n",
      "\u001b[35m third sentence.  ===>  negative\n",
      "\u001b[34m third sentence. ===>  [221, 3, 23, 445, 26, 17, 1, 298, 16, 31, 446, 613, 7, 17, 27, 59, 19, 100, 8, 1, 348, 7, 85, 44, 967]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the first three expressions of the document look like after the tokenisation operation.\n",
    "print(data.clean[0])\n",
    "print(f\"{Fore.BLUE} first sentence. ===> \",data.clean[0])\n",
    "print(f\"{Fore.BLUE} first sentence. ===> \",train_sequences[0])\n",
    "\n",
    "print(f\"{Fore.GREEN} second sentence.  ===>\",data.clean[1])\n",
    "print(f\"{Fore.BLUE} second sentence. ===> \",train_sequences[1])\n",
    "\n",
    "print(f\"{Fore.MAGENTA} third sentence.  ===> \",data.clean[2])\n",
    "print(f\"{Fore.BLUE} third sentence. ===> \",train_sequences[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will try to transform each expression in the tokenizer to the length format defined above.\n",
    "This allows us to have expressions of the same length.\n",
    "\"\"\"\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m first sentence. ===>  [444 250 346 347 961   2  18  11  63   3  23 962  39   1 963   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "\u001b[32m second sentence.  ===> [964   3  19   1   4   7   1  41  36  16 205  42   5 612 296   2   1 965\n",
      "  10   5  48  26 966 297   6  19   1   4   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n",
      "\u001b[35m third sentence.  ===>  [221   3  23 445  26  17   1 298  16  31 446 613   7  17  27  59  19 100\n",
      "   8   1 348   7  85  44 967   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what the first three expressions of the document look like after the operation.\n",
    "\n",
    "print(f\"{Fore.BLUE} first sentence. ===> \",train_padded[0])\n",
    "print(f\"{Fore.GREEN} second sentence.  ===>\",train_padded[1])\n",
    "print(f\"{Fore.MAGENTA} third sentence.  ===> \",train_padded[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the same operations on the test sentences\n",
    "\n",
    "\"\"\"\n",
    "We will try to transform each expression in the tokenizer to the length format defined above.\n",
    "This allows us to have expressions of the same length.\n",
    "\"\"\"\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m Shape of train. ===>  {(818, 42)}\n",
      "\u001b[34m Shape of train. ===>  {(205, 42)}\n",
      "\u001b[32m This means that 80% of the training data corresponds to 817 sentences of 42 words each. \n"
     ]
    }
   ],
   "source": [
    "# how our training data is dimensioned.\n",
    "print(f\"{Fore.BLUE} Shape of train. ===> \",{train_padded.shape})\n",
    "print(f\"{Fore.BLUE} Shape of train. ===> \",{test_padded.shape})\n",
    "print(f\"{Fore.GREEN} This means that 80% of the training data corresponds to 817 sentences of 42 words each. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 42, 100)           254000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 42, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 308,755\n",
      "Trainable params: 308,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will now create a model that will be adapted to binary data. That is, with two labels, positive and negative\n",
    "\n",
    "positive ==> [0 0 1]\n",
    "Neutral  ==> [1 0 0]\n",
    "Negative ==> [0 1 0]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# define model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 100, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))\n",
    "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.1))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/52 - 43s - loss: 1.0019 - accuracy: 0.4914\n",
      "Epoch 2/10\n",
      "52/52 - 4s - loss: 0.9906 - accuracy: 0.4963\n",
      "Epoch 3/10\n",
      "52/52 - 5s - loss: 0.9879 - accuracy: 0.4988\n",
      "Epoch 4/10\n",
      "52/52 - 6s - loss: 0.9131 - accuracy: 0.5844\n",
      "Epoch 5/10\n",
      "52/52 - 6s - loss: 0.7084 - accuracy: 0.7188\n",
      "Epoch 6/10\n",
      "52/52 - 5s - loss: 0.6695 - accuracy: 0.7482\n",
      "Epoch 7/10\n",
      "52/52 - 6s - loss: 0.6137 - accuracy: 0.7714\n",
      "Epoch 8/10\n",
      "52/52 - 4s - loss: 0.4651 - accuracy: 0.8178\n",
      "Epoch 9/10\n",
      "52/52 - 4s - loss: 0.3480 - accuracy: 0.8692\n",
      "Epoch 10/10\n",
      "52/52 - 4s - loss: 0.3195 - accuracy: 0.8936\n",
      "\u001b[32m-------------------  The model was trained. ------------------- \n"
     ]
    }
   ],
   "source": [
    "# now we have to train the model \n",
    "model.fit(train_padded, y_train, epochs=10,batch_size=16,  verbose=2)\n",
    "print(f\"{Fore.GREEN}-------------------  The model was trained. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model :  52.20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we test the model and we print the metrics data\n",
    "\"\"\"\n",
    "\n",
    "predictions = model.predict(test_padded)\n",
    "y_pred = (predictions > 0.5)\n",
    "print('Accuracy of the model : ', \"%.2f\" % (accuracy_score(y_pred, y_test)*100))\n",
    "# print(\"F1-score:  : \", \"%.2f\" %  (f1_score(y_pred, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- The simulation messages were successfully recorded.. ------------------- \n"
     ]
    }
   ],
   "source": [
    "#Simulate the model with unknow values\n",
    "\n",
    "\n",
    "# you cn write your own sentences on e and f nd check the result\n",
    "a = [\"a vaccine no i am not interested.\"]\n",
    "b = [\"There are times when I wonder why it is free. Anything that is free is dangerous. So i will never get it.\"]\n",
    "c = [\"I really don't know. I let time tell me.\"]\n",
    "d = [\" I don't think there's much difference with or without the vaccine, so I don't know what to do.\"]\n",
    "e = [\"I have my two doses and I am still alive. I am waiting for the others to find my freedom.\"]\n",
    "f = [\"Vaccination is very important. Also the vaccination against covid19.\"]\n",
    "\n",
    "print(f\"{Fore.GREEN}------------------- The simulation messages were successfully recorded.. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- The simulation messages cleaning operation is complete.. ------------------- \n"
     ]
    }
   ],
   "source": [
    "# clean the values\n",
    "clean_textA = clean_tweets(a)\n",
    "clean_textB = clean_tweets(b)\n",
    "clean_textC = clean_tweets(c)\n",
    "clean_textD = clean_tweets(d)\n",
    "clean_textE = clean_tweets(e)\n",
    "clean_textF = clean_tweets(f)\n",
    "\n",
    "print(f\"{Fore.GREEN}------------------- The simulation messages cleaning operation is complete.. ------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m 1.sentence. ===>  ['a vaccine no i am not interested.']\n",
      "\u001b[34m 1.sentence. ===>  [[  9   4  31   3  23   6 303   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]]\n",
      "\u001b[32m 2.sentence. ===>  ['There are times when I wonder why it is free. Anything that is free is dangerous. So i will never get it.']\n",
      "\u001b[32m 2.sentence. ===>  [[  52   16 2033   73    3  311   69   10    5  101  602   17    5  101\n",
      "     5  650   26    3   20  114   19   10    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\u001b[31m 3.sentence. ===>  [\"I really don't know. I let time tell me.\"]\n",
      "\u001b[31m 3.sentence. ===>  [[   3  131  537   79    3   95  115 1802   48    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\u001b[30m 4.sentence. ===>  [\" I don't think there's much difference with or without the vaccine, so I don't know what to do.\"]\n",
      "\u001b[30m 4.sentence. ===>  [[  3 537  56 630 141 372  32  50  88   1   4  26   3 537  79  80   2  22\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]]\n",
      "\u001b[35m 5.sentence. ===>  ['I have my two doses and I am still alive. I am waiting for the others to find my freedom.']\n",
      "\u001b[35m 5.sentence. ===>  [[   3   25   24  111   81    7    3   23  108 1318    3   23  852   15\n",
      "     1  191    2  194   24  653    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\u001b[34m 6.sentence. ===>  ['Vaccination is very important. Also the vaccination against covid19.']\n",
      "\u001b[34m 6.sentence. ===>  [[ 47   5  77 163 137   1  47  58 241   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will try to transform each expression in the tokenizer to the length format defined above.\n",
    "This allows us to have expressions of the same length.\n",
    "\"\"\"\n",
    "simulate_sentence_A = tokenizer.texts_to_sequences(clean_textA)\n",
    "simulate_sentence_B = tokenizer.texts_to_sequences(clean_textB)\n",
    "simulate_sentence_C = tokenizer.texts_to_sequences(clean_textC)\n",
    "simulate_sentence_D = tokenizer.texts_to_sequences(clean_textD)\n",
    "simulate_sentence_E = tokenizer.texts_to_sequences(clean_textE)\n",
    "simulate_sentence_F = tokenizer.texts_to_sequences(clean_textF)\n",
    "\n",
    "\n",
    "test_padded1 = pad_sequences(simulate_sentence_A, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded2 = pad_sequences(simulate_sentence_B, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded3 = pad_sequences(simulate_sentence_C, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded4 = pad_sequences(simulate_sentence_D, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded5 = pad_sequences(simulate_sentence_E, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded6 = pad_sequences(simulate_sentence_F, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "print(f\"{Fore.BLUE} 1.sentence. ===> \",a)\n",
    "print(f\"{Fore.BLUE} 1.sentence. ===> \",test_padded1)\n",
    "\n",
    "print(f\"{Fore.GREEN} 2.sentence. ===> \",b)\n",
    "print(f\"{Fore.GREEN} 2.sentence. ===> \",test_padded2)\n",
    "\n",
    "print(f\"{Fore.RED} 3.sentence. ===> \",c)\n",
    "print(f\"{Fore.RED} 3.sentence. ===> \",test_padded3)\n",
    "\n",
    "print(f\"{Fore.BLACK} 4.sentence. ===> \",d)\n",
    "print(f\"{Fore.BLACK} 4.sentence. ===> \",test_padded4)\n",
    "\n",
    "print(f\"{Fore.MAGENTA} 5.sentence. ===> \",e)\n",
    "print(f\"{Fore.MAGENTA} 5.sentence. ===> \",test_padded5)\n",
    "\n",
    "print(f\"{Fore.BLUE} 6.sentence. ===> \",f)\n",
    "print(f\"{Fore.BLUE} 6.sentence. ===> \",test_padded6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m------------------- Legend. ------------------- \n",
      "\u001b[31m negative ==> [1 0 0]\n",
      "\u001b[31m Neutral  ==> [0 1 0]\n",
      "\u001b[31m positive ==> [0 0 1]\n",
      "\u001b[32m#####################################################################################################\n",
      "\u001b[30m 1 --> display:  [[1. 0. 0.]]  instead of Negative [1 0 0]\n",
      "\u001b[30m 2 --> display:  [[1. 0. 0.]]  instead of Negative [1 0 0]\n",
      "\u001b[30m 3 --> display:  [[0. 1. 0.]]  instead of Neutral  [0 1 0]\n",
      "\u001b[30m 4 --> display:  [[0. 1. 0.]]  instead of Neutral  [0 1 0]\n",
      "\u001b[30m 5 --> display:  [[0. 0. 1.]]  instead of Positive [0 0 1]\n",
      "\u001b[30m 6 --> display:  [[0. 0. 1.]]  instead of Positive [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred1 = model.predict(test_padded1)\n",
    "pred2 = model.predict(test_padded2)\n",
    "pred3 = model.predict(test_padded3)\n",
    "pred4 = model.predict(test_padded4)\n",
    "pred5 = model.predict(test_padded5)\n",
    "pred6 = model.predict(test_padded6)\n",
    "\n",
    "print(f\"{Fore.BLUE}------------------- Legend. ------------------- \")\n",
    "print(f\"{Fore.RED} negative ==> [1 0 0]\")\n",
    "print(f\"{Fore.RED} Neutral  ==> [0 1 0]\")\n",
    "print(f\"{Fore.RED} positive ==> [0 0 1]\")\n",
    "\n",
    "print(f\"{Fore.GREEN}#####################################################################################################\")\n",
    "\n",
    "print(f\"{Fore.BLACK} 1 --> display: \", np.around(pred1, decimals=0),\" instead of Negative [1 0 0]\")\n",
    "print(f\"{Fore.BLACK} 2 --> display: \", np.around(pred2, decimals=0),\" instead of Negative [1 0 0]\")\n",
    "print(f\"{Fore.BLACK} 3 --> display: \", np.around(pred3, decimals=0),\" instead of Neutral  [0 1 0]\")\n",
    "print(f\"{Fore.BLACK} 4 --> display: \", np.around(pred4, decimals=0),\" instead of Neutral  [0 1 0]\")\n",
    "print(f\"{Fore.BLACK} 5 --> display: \", np.around(pred5, decimals=0),\" instead of Positive [0 0 1]\")\n",
    "print(f\"{Fore.BLACK} 6 --> display: \", np.around(pred6, decimals=0),\" instead of Positive [0 0 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m------------------- The firsst NN prototype is completed. ------------------- \n"
     ]
    }
   ],
   "source": [
    "print(f\"{Fore.GREEN}------------------- The firsst NN prototype is completed. ------------------- \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
